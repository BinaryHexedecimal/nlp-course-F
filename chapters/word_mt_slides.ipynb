{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0e1b904e914a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatnlpbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatnlpbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatnlpbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_log\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plt'"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'UniformLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92b698c4c9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniformLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UniformLM' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec27faf13d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# measure_change(align_matrices, align_matrices)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mibm1_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem_model1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mibm1_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchange\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mibm1_iterations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'UniformLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1d539946b3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"groß\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ein\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mann\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtarget_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_model_2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniformLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_vocab\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'NULL'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mibm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UniformLM' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LaplaceLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-114277d40331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_model_2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLaplaceLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNGramLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mibm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LaplaceLM' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util\n",
    "import statnlpbook.mt as mt\n",
    "util.execute_notebook('word_mt.ipynb')\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Translation\n",
    "Consists of learning bilingual correlations between two different languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Why is MT a challenging task? \n",
    "\n",
    "* Because languages are hard even for humans! \n",
    "* What's the Danish equivalent of: **''It is all Greek to me''**? \n",
    "\n",
    "![greektome](https://lh3.googleusercontent.com/-mQiInH86oo4/Vrtcx9p9QcI/AAAAAAAAxGg/OKJZKe0P4c0/w530-h849-p/greektome.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  MT epic fails\n",
    "\n",
    "![1](https://static.boredpanda.com/blog/wp-content/uuuploads/funny-chinese-sign-translation-fails/funny-chinese-sign-translation-fails-15.jpg)\n",
    "\n",
    "\n",
    "![2](https://static.boredpanda.com/blog/wp-content/uuuploads/funny-chinese-sign-translation-fails/funny-chinese-sign-translation-fails-26.jpg)\n",
    "\n",
    "![3](https://static.boredpanda.com/blog/wp-content/uploads/2016/06/hilarious-chinese-translation-fails-english-49-5768e94085a80__605.jpg)\n",
    "\n",
    "![4](https://i.pinimg.com/originals/6f/ef/ee/6fefeee695c849bc1727ad92464fd111.jpg)\n",
    "\n",
    "![5](https://alvinology.com/wp-content/uploads/2014/03/2691947209_4e4fe0d260_o.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  That does not happen only with complicated languages like Chinese but also ...\n",
    "\n",
    "<center><img src=\"../chapters/mt_figures/avocado.png\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Machine Translation (MT)\n",
    "\n",
    "* There are four paradigms in MT: \n",
    "\n",
    "  * Rule-based machine translation or RBMT: \n",
    "          is based on linking the structure of the given input sentence with\n",
    "          the structure of the foreign output sentence. To translate a sentence from French to English, we need:\n",
    "            * A dictionary that will map each French word to an appropriate\n",
    "            English word. \n",
    "            * Rules representing regular French sentence structure. \n",
    "            * Rules representing regular English sentence structure. \n",
    "            * Rules according to which one can relate these two structures\n",
    "            together. \n",
    "  * Example-based machine translation or EBMT: \n",
    "          is trained from bilingual parallel corpora containing parallel\n",
    "          sentences in a **specific format**, the training sentences differ from\n",
    "          each other by only one element which makes it easier for the system to\n",
    "          memorize. For example, if we train on the two sentences ``Good\n",
    "          postgraduate students in the CSE department are smart'' and\n",
    "          ``Programmers are good people\" we would be able to translate\n",
    "          ``Programmers in the CSE department are smart\" by just substituting\n",
    "          the appropriate parts of the sentences.\n",
    "  * Statistical machine translation or SMT: \n",
    "          consists of applying advanced machine learning algorithms and techniques to learn bilingual\n",
    "          correlations between two languages by training on a parallel corpus.\n",
    "          The parallel corpus is generally a large number of parallel sentences\n",
    "          (bi-sentences), humanly translated.\n",
    "  * Neural Machine Translation or NMT: \n",
    "          is a new architecture for getting machines to learn to translate by modeling the whole translation process as one big artificial neural network. The most common models in NMT are based on the encoder-decoder architecture     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Statistical Machine Translation \n",
    "\n",
    "Translate **word-by-word** \n",
    "* **foundational** to all current approaches (e.g. neural methods)\n",
    "* **subcomponent** in more complex systems (for alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MT as Structured Prediction\n",
    "\n",
    "* **source** sentence \\\\(\\source\\\\)\n",
    "    * aka $\\x$, usually tokenized\n",
    "    * \"音楽 が 好き\" or \"Ich mag Musik\"\n",
    "* **target** sentence \\\\(\\target\\\\)\n",
    "    * aka $\\y$, usually tokenized \n",
    "    * I like Music\n",
    "* a **model** \\\\(s_\\params(\\target,\\source)\\\\) to measure match of \\\\(\\target\\\\) to $\\source$\n",
    "    * $s($I like Music,音楽 が 好き$)=12$\n",
    "    * $s($I like StatNLP,音楽 が 好き$)=-20$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A translation pipeline is generally composed of: \n",
    "\n",
    "<center><img src=\"../chapters/mt_figures/pipeline.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For most of this lecture, we consider a German-English translation task. This means that:\n",
    "\n",
    "* **Source**: German\n",
    "* **Target**: English\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n",
    "learn the parameters \\\\(\\params\\\\) from data \n",
    "* usually from **parallel corpora** of **aligned source and target sentences**\n",
    "* Where can you get such data from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Translation\n",
    "predict highest-scoring translation given source $\\source$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\argmax_\\target s_\\params(\\target,\\source)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MT approaches differ primarily in \n",
    "* how \\\\(s\\\\) is defined, \n",
    "* how \\\\(\\params\\\\) are learned\n",
    "* how the translation \\\\(\\argmax\\\\) is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelling\n",
    "How to define $s_\\params(\\target,\\source)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models \n",
    "How is the $(\\target,\\source)$ data **generated**? \n",
    "\n",
    "Develop a **distribution** $s_\\params(\\target,\\source)=\\prob_\\params(\\target,\\source)$ that generates **faithful samples**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Good** generative model: samples look like\n",
    "* I like music --- Ich mag Musik\n",
    "* I love music --- Ich mag Musik sehr gerne \n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Bad** generative model:\n",
    "* I don't like music --- Ich mag Musik\n",
    "* I love music --- Musik sehr gerne ich mag\n",
    "    * *example is a reasonable translation, but very unlikely German.* \n",
    "\n",
    "Generative models need to model both input and output! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we decompose the problem?\n",
    "\n",
    "*in the same way we decomposed language models $\\prob(w_1,\\ldots,w_n)=\\prob(w_1)\\prob(w_2|w_1)\\ldots$*\n",
    "\n",
    "Who remembers the Markov Assumption? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Think about the problem **backwards**\n",
    "* Generate the target $\\target$ using $\\prob_{\\params_t}(\\cdot)$\n",
    "* Generate the source $\\source$ using $\\prob_{\\params_s}(\\cdot|\\target)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The translation system can be modeled as 2 compounds:\n",
    "\n",
    "* Translation model or $\\prob(s|t)$: \n",
    "    assigns a conditional probability $\\prob(s|t)$ to any foreign/English or source/target bisentences.\n",
    "    based on lexical translation and the notion of alignment\n",
    "    $\\prob(avocat|lawyer)$ or $\\prob(avocat|avocado)$\n",
    "\n",
    "\n",
    "* Language model or $\\prob(t)$:\n",
    "    * helps to ensure fluency by using n-gram approximation: the next word can be predicted using a short history (one or two words)\n",
    "    * t=I ate an apple            $\\prob(t)=\\prob(I)×\\prob(ate│I)×\\prob(an│I,ate)×\\prob(apple|ate,an)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This defines a **joint** distribution $\\prob_\\params(\\target,\\source) = \\prob_{\\params_t}(\\target) \\prob_{\\params_s}(\\source|\\target)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n",
    "Find $\\params$ using **MLE** (or smoothed variants)\n",
    "\n",
    "$$\n",
    "\\argmax_\\params \\sum_{(\\target,\\source) \\in \\train} \\log \\prob_\\params(\\target, \\source)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Translation\n",
    "Operate **forwards**:\n",
    "\n",
    "$$\n",
    "\\argmax_\\target \\prob_\\params(\\target|\\source) = \\argmax_\\target \\prob_\\params(\\target,\\source)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why this equality? $p(\\target|\\source)=\\frac{p(s,t)}{p(s)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How important would $\\prob(\\source)$ be if we had chosen $\\prob(\\target,\\source) = \\prob(\\source) \\prob(\\target|\\source)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAAnEAYAAABxl8L2AAAJJGlDQ1BpY2MAAHjalZVnUJNZF8fv\n8zzphUASQodQQ5EqJYCUEFoo0quoQOidUEVsiLgCK4qINEUQUUDBVSmyVkSxsCgoYkE3yCKgrBtX\nERWUF/Sd0Xnf2Q/7n7n3/OY/Z+4995wPFwCCOFgSvLQnJqULvJ3smIFBwUzwg8L4aSkcT0838I96\nPwyg5XhvBfj3IkREpvGX4sLSyuWnCNIBgLKXWDMrPWWZDy8xPTz+K59dZsFSgUt8Y5mjv/Ho15xv\nLPqa4+vNXXoVCgAcKfoHDv+B/3vvslQ4gvTYqMhspk9yVHpWmCCSmbbcCR6Xy/QUJEfFJkT+UPC/\nSv4HpUdmpy9HbnLKBkFsdEw68/8ONTIwNATfZ/HW62uPIUb//85nWd+95HoA2LMAIHu+e+GVAHTu\nAED68XdPbamvlHwAOu7wMwSZ3zzU8oYGBEABdCADFIEq0AS6wAiYAUtgCxyAC/AAviAIrAN8EAMS\ngQBkgVywDRSAIrAH7AdVoBY0gCbQCk6DTnAeXAHXwW1wFwyDJ0AIJsArIALvwTwEQViIDNEgGUgJ\nUod0ICOIDVlDDpAb5A0FQaFQNJQEZUC50HaoCCqFqqA6qAn6BToHXYFuQoPQI2gMmob+hj7BCEyC\n6bACrAHrw2yYA7vCvvBaOBpOhXPgfHg3XAHXwyfgDvgKfBsehoXwK3gWAQgRYSDKiC7CRriIBxKM\nRCECZDNSiJQj9Ugr0o30IfcQITKDfERhUDQUE6WLskQ5o/xQfFQqajOqGFWFOo7qQPWi7qHGUCLU\nFzQZLY/WQVugeehAdDQ6C12ALkc3otvR19DD6An0ewwGw8CwMGYYZ0wQJg6zEVOMOYhpw1zGDGLG\nMbNYLFYGq4O1wnpgw7Dp2AJsJfYE9hJ2CDuB/YAj4pRwRjhHXDAuCZeHK8c14y7ihnCTuHm8OF4d\nb4H3wEfgN+BL8A34bvwd/AR+niBBYBGsCL6EOMI2QgWhlXCNMEp4SyQSVYjmRC9iLHErsYJ4iniD\nOEb8SKKStElcUggpg7SbdIx0mfSI9JZMJmuQbcnB5HTybnIT+Sr5GfmDGE1MT4wnFiG2RaxarENs\nSOw1BU9Rp3Ao6yg5lHLKGcodyow4XlxDnCseJr5ZvFr8nPiI+KwETcJQwkMiUaJYolnipsQUFUvV\noDpQI6j51CPUq9RxGkJTpXFpfNp2WgPtGm2CjqGz6Dx6HL2IfpI+QBdJUiWNJf0lsyWrJS9IChkI\nQ4PBYyQwShinGQ8Yn6QUpDhSkVK7pFqlhqTmpOWkbaUjpQul26SHpT/JMGUcZOJl9sp0yjyVRclq\ny3rJZskekr0mOyNHl7OU48sVyp2WeywPy2vLe8tvlD8i3y8/q6Co4KSQolCpcFVhRpGhaKsYp1im\neFFxWommZK0Uq1SmdEnpJVOSyWEmMCuYvUyRsryys3KGcp3ygPK8CkvFTyVPpU3lqSpBla0apVqm\n2qMqUlNSc1fLVWtRe6yOV2erx6gfUO9Tn9NgaQRo7NTo1JhiSbN4rBxWC2tUk6xpo5mqWa95Xwuj\nxdaK1zqodVcb1jbRjtGu1r6jA+uY6sTqHNQZXIFeYb4iaUX9ihFdki5HN1O3RXdMj6Hnppen16n3\nWl9NP1h/r36f/hcDE4MEgwaDJ4ZUQxfDPMNuw7+NtI34RtVG91eSVzqu3LKya+UbYx3jSONDxg9N\naCbuJjtNekw+m5qZCkxbTafN1MxCzWrMRth0tie7mH3DHG1uZ77F/Lz5RwtTi3SL0xZ/Wepaxls2\nW06tYq2KXNWwatxKxSrMqs5KaM20DrU+bC20UbYJs6m3eW6rahth22g7ydHixHFOcF7bGdgJ7Nrt\n5rgW3E3cy/aIvZN9of2AA9XBz6HK4ZmjimO0Y4ujyMnEaaPTZWe0s6vzXucRngKPz2viiVzMXDa5\n9LqSXH1cq1yfu2m7Cdy63WF3F/d97qOr1Vcnre70AB48j30eTz1Znqmev3phvDy9qr1eeBt653r3\n+dB81vs0+7z3tfMt8X3ip+mX4dfjT/EP8W/ynwuwDygNEAbqB24KvB0kGxQb1BWMDfYPbgyeXeOw\nZv+aiRCTkIKQB2tZa7PX3lwnuy5h3YX1lPVh68+EokMDQptDF8I8wurDZsN54TXhIj6Xf4D/KsI2\noixiOtIqsjRyMsoqqjRqKtoqel/0dIxNTHnMTCw3tir2TZxzXG3cXLxH/LH4xYSAhLZEXGJo4rkk\nalJ8Um+yYnJ28mCKTkpBijDVInV/qkjgKmhMg9LWpnWl05c+xf4MzYwdGWOZ1pnVmR+y/LPOZEtk\nJ2X3b9DesGvDZI5jztGNqI38jT25yrnbcsc2cTbVbYY2h2/u2aK6JX/LxFanrce3EbbFb/stzyCv\nNO/d9oDt3fkK+Vvzx3c47WgpECsQFIzstNxZ+xPqp9ifBnat3FW560thROGtIoOi8qKFYn7xrZ8N\nf674eXF31O6BEtOSQ3swe5L2PNhrs/d4qURpTun4Pvd9HWXMssKyd/vX779Zblxee4BwIOOAsMKt\noqtSrXJP5UJVTNVwtV11W418za6auYMRB4cO2R5qrVWoLar9dDj28MM6p7qOeo368iOYI5lHXjT4\nN/QdZR9tapRtLGr8fCzpmPC49/HeJrOmpmb55pIWuCWjZfpEyIm7J+1PdrXqtta1MdqKToFTGade\n/hL6y4PTrqd7zrDPtJ5VP1vTTmsv7IA6NnSIOmM6hV1BXYPnXM71dFt2t/+q9+ux88rnqy9IXii5\nSLiYf3HxUs6l2cspl2euRF8Z71nf8+Rq4NX7vV69A9dcr9247nj9ah+n79INqxvnb1rcPHeLfavz\ntuntjn6T/vbfTH5rHzAd6Lhjdqfrrvnd7sFVgxeHbIau3LO/d/0+7/7t4dXDgw/8HjwcCRkRPox4\nOPUo4dGbx5mP559sHUWPFj4Vf1r+TP5Z/e9av7cJTYUXxuzH+p/7PH8yzh9/9UfaHwsT+S/IL8on\nlSabpoymzk87Tt99ueblxKuUV/MzBX9K/FnzWvP12b9s/+oXBYom3gjeLP5d/Fbm7bF3xu96Zj1n\nn71PfD8/V/hB5sPxj+yPfZ8CPk3OZy1gFyo+a33u/uL6ZXQxcXHxPy6ikLxyKdSVAAAAIGNIUk0A\nAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAAGYktHRP///////wlY99wAAAAJ\ncEhZcwAAASwAAAEsAHOI6VIAAAAHdElNRQfiChIMAzu0xNQfAAAv9klEQVR42u19B1zN+///+Zw6\np61FGlRKNIwoo2EmVMZFCNluVOaNZI/KRdkZyV7XioRsHVRmuvY1QkOD9jTC/36en7ffv+N7nFvJ\nveH9uo+H5+Wc8/7s1+v1fK0Pj0eFChUqVKhQoUKFChUqVKhQoUKFChUqVKhQoUKFChUqVKhQoUKF\nChUqVKhQoUKFChUqVKhQoUKFChUqVKhQoUKFChUqVKhQoUKlpgnTgvyPWxUXaEywhJ5LKlSo1DD9\nNoT8T5uvXEiG4AmCLapp/0zoNaJChQoVKj+RGLzjsNt2DgX+VVuHP4VDG1sOjZPpuaVChcp/KyYD\nOOwygvCHSV9JFFpzOHgkh3qvqmc/zVZwaL2X/MNZeu2oUKFChcoPKPU3c/gbyXgY7yQffJ4BecCB\n0hZCNOp8YUGSAalHCIzPew71Q+m5pkKFyr8rDUlGYcpzou9yyQdfmQFhLnC4ZA2HFg7Sv6+UwaHg\nH9ZVacmhVwjZzU30GlKhQoUKlR9IBIYcTl/HYZdw6d/XeMThWksOtdZWbDsORWQ7PTiUnU7PPRUq\nVL6tyGtwOLchh7YR1bs+c4rD3305NN8u+XsywRyu0iWE4tcKEid9sn46h5pCek2pUKFChUoNEuaX\nLxi+f4jwWZoSw5jEoeLcL3zxGgf91Di8I8uhLsmcMF7ke70k/1zpGCEupzlsOoReMypUqFRQv/l9\nQb9ZS/+dbTyHQbM4FG6o2Pb4iwkWVpKA7JD8vVYXOXxMsCPJJDNkv3i/fWE/BnM4y5Lo3zB6L1Ch\nQoUKlf9QZBQ4bEsibl4LiGEjBtf+DTFYpQTVOVS4IL6ONzHgv5VK3o6QrOtuwOF9EkksbMvhljgO\nF5OagvpK0vd7JsmYjLpFryEVKlS+4HiTUqXWDTj0jOCwiyYhFiSw0vcFh66JHCpbiK/jSxx9j3jp\n21NV43AYcfB993E4jei5QJL57XL2HwhIT7L/ZL97ET13iTSnvycBn0Ok92R5Iw6b/EOJVR9S6rp4\nG9H/7vQeoUKlmuXjR4oUqw9/XDEihrgnaX70IdOmjupxaDKKw0+9GivyicH+LEMRQgz2L4++QHQm\nc2ixnMOLSzjM0OKwW1MOm5NInqKB9P0eRAjL0nlUn1GkSPWkZNEnvWN9iX7yJkQjijR9m5YRfUP0\nyBLSezHoICEGTTjcTBz3Ll7Stzd8NIeHnxJCsIcEVFI49CO9a+7MPxAQUlrKeBA9TXCjPIfvAjgc\nQ4Z9tCD6WfWk9P2zNORwDyndUjKneo8ixWrWgx8/+ogoUqw6jrHmUO3cj+3aGpDmcEMS+Vs2jsOZ\nCp9FEsdwuOlTKr+h+Of7SMlVBw3p25MhBCXiHodJxHDW6Vq5/e5BHImtXXk/gXz8yMzgsGshfT4p\n1hwcPIdD+YY188nRI5kGI1LqGUgIycI/PyMAlhyuJhmGQEJMZK8QfZXGYcuZ0rfnTvRaOnHso1tx\nuP4lIQwko6E74bPtn/mMgOySvP6cbA7fkoyzXSUJhBE5/ijiKqluqNl6z+A+h+6hBGdRpFgD8QiH\nfS+TG5fjIhQpVg2TlDg0Wc/7KURlIIdHpnHo+Eb8cw0yjSr6KocDPjN8O4khcwiUvh0hiSQeeUsI\nCOkZ0SKZFSUXDuu+l75Ob+JYhHb/OQiITG8Ot7rR55NizcHrNhxqdKvZT5ASGeN9sDOHPVeKf16L\nEIzTJEM79AgJvJASrQNTOWxdJn07tYljP86VBGYIJpASqgxSOjWdnC9mKcH1nxGQLzShzyPE5C35\nvT3R26qRHGqekb5/Jvs5jCSErJZ7zdZ7A/w5fDeYYG+KFGsgjiZ+ozMlIBSrARM3EQIi+3MQEIu6\nHF4mU6wafBbp67+Iw70HiKH9rATLnxjS4WOlb0dAMiWHyRz9F6QUS5tkWLoTgzh3ifR1xhJCNPP3\nn4SAdOZwUyZ9PinWHLxiQAhIDX+CjGtzGOPIYZMk8c9dSI/H/m0kAHKcfED00jKi9/r9gz0YlsWh\nB9Fz8qQpXI2M7w0kBGMf0bMCMqacsa8YAZlN9HIZKclqTwjJMD6Hnv+QEWlHxvxuJKWucgdqtt5z\n60WfM4rfD6YJKQGhSAlIpcWNlFzFx3I4kBjgrmTc48zdHJqNkvz7Tz0h/r8Qg/qFJvJPb+YdtpHD\nW2ocfppqteZTrfUXInmMHIdLiAF3+YsSEIoUKQH5h4wpyUAkRJMMBymJ6kKGWfiS3rdm8l8gFrM5\n9PvUA6Iv+Xu/EX12lrxAddJwcUKynDj+PT6NKbcheu1YxQiILZkWeL0ZIUykd2Q9eR9JWxnp52FU\nFIdTH5J/yKvhBMSQPmcUvx98EUwJCEVKQCosDGm2DCbEYeEHkgEx5LCRNylhGCB9nbpk2ssq0jxZ\n/x/G48oYcWjYl0MrEjk0IE2UjJHk3+mTUq01Thxq/SRN6JSAUKQEpApC9IU/CagEEQJiSIZuNLrJ\noXK69GUMSOBkJdFTWvUkf6826SHRJxncJuR7Lcj4cV0SQPl8zHhF3wPCI/pOl2QyrEhJl8GnMenH\nJf9MkfTuBSmT3935PvQeJSAUKQGhSAnIjyZk+pQKKXmKIgasz5gqrkcyJb1JpsTjMIfV9aJAPik1\nGEGaL3vr8n4ioQSEIiUglRKSgVUi7984TKZgDe5QxfVIj4UbyVQMIxkO/rJqCgR9IiBkKId5ZPWe\njk4xHHqTDI1sISUgFCnWYAKiHaMdw+JQ3lD8fWW/lf1Y3Pq3sLj8w/IPLA72GezDos4rnVc/8wXQ\nT9RPZNGqk1UnFhXWK6ynBKSmipDURLvZcXikDykZIM2XqlU01EIy3rE/iQB2JVNh+FU0eAyJFLYi\nJQf9MzmU+ykyH9+SgChEKUSx2Nm0symLc7vO7cri5pDNISyGbg7dzOIU6ynWLDbzaObBIv8g/6C0\ndeuPqT+GReub1jdZlM+Sz6KGqeaghq2GLYuto1tHs6ipo6nz4xEQGdIT0Y9kJA6RDMe0+hxqzq/a\nugrEgR9Mpvl1JPqTuf+VBISURgWQzK9Zz+o5D4YkoDSc9LhoLvm+9F7VCYhKVxXos9bprdNZbLup\n7SZgbNtYadh6fWv4LcaDjAexKIgSRFG98f+Rmc/MZ9Ei2gL6w9zO3I5FGQUZBXp+qoGA2HnZebEY\nqxyrzGJR+6L2LD6a92gei9dGXBvB4oPCB4UsFpwrOMfi5aGXh7JoG2YbhvVkeDI/0wVYVLKohMXn\nxs+NWTTLM8ujBKTGymOSmielUoqEkCgRIsI3+brlBWR8sRGZHiNzsIqGeRuHemQ+v1ws7yeU6iQg\n+qb6IBxb3m55y2Ju09ymLCb/lvwbi9dfXn/J4p29d/bic5NcE3x+N/kui55XPa/CMG8WbJa0/vzf\n5//OYtrLNKzTWKaxDDVMNQd7H+99HNd1bO5YFvv59/P/ATMg44leMyRIMqdKpHSTf/7rlpcjhKMB\nCYjwnb5yf8n0QJMbZD+Dquc01CHj1VWDvk+9V3UCYqtsC/8tzzoPAZRi9WJ1FvPv5kOP5WfnZ4vh\np383zDdk8enup7tZ3LR602oWTfgmfKo/Pn4UHhQiAHXR96Ivi2fOnYH/qzJHZQ49P19BQOqY1TFj\n8ULkhUgWs1tkt4DB/fs/VgzqGNRhUW2W2ixE+lrXb83iyKYjYcDTLNMsWYyZGzOXRa1Yrdif6QIE\nywfLs5gRlBEEhuxv4U8JCBUqNYOAqF5QvcDiH+F/hLNY8kvJL3huRwePxvPKt4CBVc9Xz0cGeKX2\nShYd/v6PFVG8KB6O6+vc1yz2WNpjqaTtBA4KROQwd3UuDLfpXNO51DDVHOwb2Rf27fXI1yNZHLBr\nwK4fvwmdys9GQOxt7G1YLFtVtorFsxPPTsT9f7HvRRDvaf2mlcdP/z4iYUQCiwdnHpyJ398pu8Pi\n3oi9ESwqC5WFP7P+kD0te5rFJTeW3GAxIC4gDhl1XwVfql+/goB0edrlKRTznNdgchsbb2yMkoPz\n/PNSf6/CU2Fx1u1Zt1l82/4tMiYupi6mFUoVClQEiExO0Z8C/Ps//LuXitfXnBAZTxkwJ53lOstZ\nNFQ2RERAZb/K/kql3DozncsTKsMYQ5SmaTbRbFL+e0GOQY4gIO0y2sGh8bHwkcqkWwhB8HTX6K5h\nsYFCA6TwNAdpDvqa4+YX84tZ1PPRw/ZVbVRtKAGh8tMSED2eHoujFUbj+XrzyxsQj7Wz1iKQotBG\noU1F1ml9tPVRBGb0srFe5LDIYSwqrlFcI0ZAzAIRyMl1ynVCBkS2sSwMl4os9KROpk4m9Mh+Q+gh\nDZ5G5UoAFBlFFmsfq30M66QbosRCP1Yf+knNRM3kqwzsbVnocb3VeiBQBlEGKMFQuapy9WvW1XTT\ndMP+phimsFjXuC4yxYwao1aZdeTuyMEhqjeyHgiEoZMhzrO6q7prhQhISF+U2L2Wf42A0YDAAYGU\ngFD54QiIsT2er/eF71GpslF3o25lfq91TguR/dgLsQjc5DzOecyi5QHLAxX5vdoaNehFQ29Dbzyv\nQ+uhQkb+nPy5qhyPUoZSBvTcOP1xwJb6LfHv9ZTqVUp/+jDwi+rsrLMTfpdVAysEnN5pv4PfeEnm\nktR1THjQr7JtZdvi+/kyCFjxuvG6/RuOvmKCIghi/fX1USpncMbgDIu1xtQa8zXrChYKFuI6rai3\nAkTTS9nrXyUgrtauSNV9kqC3QW8r8/umU5tOZTGsUVgj1FbHdpaYAZEdLjucxT42feAYnxp4aiBK\nHGKT8f1U01QQl9NTToOQuAa7BuN3XrIST4hdvB0ik4tfLUYPipm9mT2L/o/98cDc176vjQxN/bT6\n2J7FKQsQrkldJuGG7MP0kdgDE6cNZrvg3QLcmLdVb6uymH4y/SSLV/dc3YMemX5D0RuzsvFKELb0\nW+m3pBEQy/6W/dFL47MVnz+a9ggRiIx+GVjnWvtrIHDTrKfheqjbqttKWqeRViMtXKfbQXAYWvZo\n2YNFr5Ne2L/EnMQcZLD+FkpAqHwfYkFKMJokf0ZAelSVgCg1VsJzeWbjmY14Pueno4a3mX4z/Uql\n3hcKoaDXXl57mcUo1yg4vLqtdFuJERCLQOiX7AfZD1hs/7Q9AjvBnsF4Du+/uP8C+zEtHc99nFKc\nEotDzIaAuMjWk5VoUHXW68DgLJy+cDqL8QPiB2CdtPQ0FlMCUgJQGrDu4joWxwWOg2OtMFthtqT1\nnHY6wQDPT52fCr3pbQZHYfnR5SBaj3Y9QmbgxbwXKL09aXwSDk0XQRcEjHiveGI9f9pztBG4WnJv\nyT0c97b221h083TDcV+ZfQX7kd41HbXpt61vQ79NV5kOYqZiqyJRz/Hm8LCubQtbBGz+mPcH9ufp\n6KfIXKVdSIODFLs0FhkprwAvnAelcKVwSkCo1GzRIFO8Oq3mkN/oMwLS4KsJSPL7ZPhlY8Kq5KCG\nvgl9w+IHhw9IBbd72O6hRMc4TBGl9x6vPKAXYmRjEHhJK0wDAXo64+kMFveb7IcDb//E/kn5gMr/\nBFJH8UdBTxU4FbB4bOsx9B4nb0uGXkn2SYb/dDT+KPy/7vHdgfzx/PES/TkrbRANf0V/bC9hRQIc\n7YxVGcgQPVB+gAD1mp1roBcbtWzUUmJg21YGemqYYBj04OCcwfCz5G7Jwe8bunsoStcWb16MEl3d\nEbojpAbKD8qgpGvsr2N/xf5N9YcfXdu6NvQjvwO/A4vdNnVDD0+kfST82+cvn6PE98UfL/5g8bz5\neXPYkalD8HvhYeFhSdvraNoR/vXvHX/viNK6YyYIZC3xWIIex2clz9BK0D+9f/q/SkDaCtrihOYE\n58Dhf2b4DLWAQy2HorRK66EWbjxmH7NP6gk9JwOGy9xgboh9vp+HiN+Qk0PgIGdnZaM58+aUmyAa\n8/3no2TJ38kfEa27qnfh8GenZ+NEDJ0zFIaImcHMKL+uJ88TWLy6GBG7Xdt3bWdxZ9xOEIhhl4fB\nYZifNx89GTkbc+CIJMxOgEGsp1ZPLAKnXKiMB2ab1jY4+K//FjwA9Y+BwPiZ+OEBWt91PWdIX93G\nA/dw1UPcyKlhqWGSCEiTpU1gIP90+dMF3xuSOgTERX0lajOnp06HI3Bi1Ak8eCVyJXIshpwOQcpP\ncYyimAJxUHfA74r+KvoLhKb21tos3nt9D/t7wvAErl+3h90eVoGAhHHY8Bg1ElT+PbHN5nDdOw7r\naHH3IcMnBORlZfVak3dNEEDIiMlA5vL4mON4jhRiFapUIipnJgeioLRJCQaBP5Q/tPznAWEBeP6L\nw4qBJ9+ffM/i9qDtKM0cNnnYZOiRX/yQiUm8mojMQoZsBgy2zVUbsUyD3AS5CXAEBoYiUFO6sBRE\n6OCBg4hETus0DUMv5i+cj39PGJOA4yv+vRi9KAOcBzhjPXmefPl1F+gtQCYnq3lWcxb3jd8Hw701\naCv2c2jPoT3R29Z/EQImuVtyt2D99ATo43rv670vv56FgYUBasjt8tGUeWTYEWSIIiMiUboxcdHE\nRTC0z8Y+Q6BlybUl0K/vXuP6uLdxl5iJsltqB735xPkJjiPRJBH6d/G9xSA6s1rMAjER5YsQiSx2\nKYZ+XSC/AMcraCJo8g0JiB6H6tvps0ul8qLZhMP1JOBiIy9OQAbmfjUByXoPP2tj2MawSum5pnIo\nrT+y7ggCGpkZmchAWDS0aCgWOQ8WwF+cO34u9EdB0wL87nThafhRfpf8kFFYFrsM+jbZNRmBm6eu\nT4HtNdprSNp+tznd4O+lXUrD7xOsEkAg5lnPg2O+yGAR9E2ibiIyO6l1U+tivSntp4hlYkRqIhb3\nnNwDvzNvaR70yZ7FexazODV9KvTZlnVbcJx58XkgMhdtL4JoGGYZig0REd4RIgMbtz0Ofmb05Wj4\nl0ojlEA0JttOxu8+eH5A4MV9pvtMaefZONc4l8WkV0nwI4+/Pw69qtBPAQHpHkN7wL6kN0mHHrvj\newelXgtDF4bifKjNg/96w/4GiEnBrAJk9sfHjYf/K2MsY1x+e5MPTwYxyR+TDzuxxXgLPn+09BHO\nS/i6cJwHW1Nb03+VgCikKsABDu4YDGZUalWKC/7O7h0Myi3FW2COoe6h7uj9UBiJkobml5vjAiib\nK5tLW19ntw6Y4Z/MnwyaPe9dhwFpeKfhHUnft7C0APFJeJqACOJtp9sgJrrZutnlv+fRy6MXLvik\nD8hohH9E4OujZqpmqljqLY1BpHBFyQowvCLjIpz4jq86ikXyeh7oCcNe/Lz4Oabi8DejNlytg1oH\nMaLVUQbnaZjXMGRmilOKUVqQKp8Kw2ahbgGCwA/gIyK3dtdaRBRz9HMQeXXe6gxGz9gz9uXXVf9V\nHUx4i9MWHG/RliIY/u4R3SPKf6/T+U4ojSuKK4orPyTAubkzHArhCiEYPu8Y71il74fMRIyRTQ0w\n8+NUoZC88E6YRJHit0Ml0jT6G5ketiKV/VOrtB5emPbx1SaDyuo1xwBHPH9vh7zlCD+zEvqHl8BL\n+BYp8gCDABjGj4kfMRVvp/JOrvSzk0onSd8f7jwcjvW7vu/6wkHXGSs2lanxhMYgIKm3UhFhi5CJ\nQFO7yhuVN5LWa+PcBuvlKedhuyudVkKP8N7wxL4/O2U29NUH2w8wmGGDwlD6qZSvlC8WiYzgQ+8s\nn70cAZvXKq+RsbAV2YrKf8+8i3kXbHdQHtb56+BfiOxZdbXqKmk/O0R2QC9GQdcCfL762uprYvYo\nTAEOU7hjOEpbU5+lgrjYnbA7ITHCKaON8xJhGQG78dLqJexXq/mt5n8zAhJ9BUMr4jTrkiERwiL6\nHFOsBJKpi13IlMNPb4i3fsv++e7t/70J/UWVCYjZewRM9i3btwwO71RjRMiN3xu/F8PxxiAQzXY1\ng5/il+4Hxzx9Vzr+vvgj/PWPcgI5QfntWP9mjeEdmfGZcNz3Dd0Hh1lDWUNZbL+SeEnIoI7vgu2k\nl6aXsnhI7RAcaIVFCghQ1LpWC3rgrNtZlGze63WvV/kM7efH6eLsAn1XHFKM5zpELgSBW8aMwXEP\nCRsCPVJypwR+5uxOs6GH5S7IXRAjUmECfM97ojd6ZV6bv4Y/O//yfPi3TBlTBr9qtRCB7rg+caic\nifaJRqBZqYFSAwS8/JsgkJ7ZPhOVLLsm7IL+FswQzJC0/yOnjsT1KL1XCn94eJ3h6LWu5VML615o\ndwEl/fcy76F01yLEIkTSOkbLjdBqEHM+Bn7hU/en8NNNzpmIlbxN8J+A/Xsf+h4E5lLWJRCsVidb\ngaDJashyhHApb+l/MgVLKVkJKTu3kW6osd3xZgcM1wO3B7ghioOLwXg/HPsAxzbzUeYjlDYNOgXD\n4zbKDRF8OUaOEZs+ktMbqaqSAyVw8D32eeyryP5MTp6M/SkuKEYqrtvYbmPFCMhWDzjyb4+9xf6M\njB8ZL229sVPH4oIXlRbhAXDs5dir/OchHUJANLJdshFJa5nUMklqc+szVRjGS/UuoXQirW8aHAmz\nK2ZXQJi66aI28EnDJ4gc7NXei5IwfkN+Q2nrWj+1BvHKdc1FpMD/vv99SQSkpG8Jtrdh0wZEZJlG\nTKOvdaQ+2DxDathx4OIDnEIMI+Nlw/ZQpPgN8RcOt/lwmIgXqcm2jcQc/+Q/D/et7P3cO713evnS\n0gC9AL1vWaMbcDoAGcvSm6UYw+uy2GWxtO934nUCFt4txBQa79neYiVT2hnaiDx65XohUtZGu422\ntPVaqrQEQcgalQU9vLb9WhhCpjfTu/z35rjNgT5/M+sNImbOV5yvSFvXa5YXvlf6aykCJO1C24WK\nERBHcxCF/Ix87O/WLVsROGESGIlEz7iHMUpGk24m4Txt6L1BbP8sjlqgFCxzfCYcllVDVg2pyPl3\nMXeB41B8rRiOjHeSt5j+7rus77LqIiBvZ99YxGKDzUvIe4U2mdLnmGIVkLzB/VoYh7cm4W4at2I3\nd799mFBVAvKm7xv4B0WJRQiIpPRJgeOcsjNlpxj2TEHGMzMtE4Ha9zfeo4JlV/NdCGiq/aX2l9h2\nLHkg+n5r/dbiud+djwBz+4z2GRXZv62OW7nAwvXU6+hlaGAAB76VVysEdPPm5mF4xzyFeVLH2ypP\nVYY/tyFnA/zLwDaByKQq71TGce2z2YdS/0dzH2G9erb1bKWtV0uhFrZ33fY6vnfuzjkQFyUfJRAC\nQb4AAZq4FnHIvEa7RsM/UxmsMhgEpaMQgemDPgfx/WcnniFgYpRkJKaHBDsEO1CS1m8/Mh2Pgh4h\n86xXpgeiY3/CHr8rVC9EINtvpN/IipzXEaNHoDS1+EjxEbQwHHE9IkZALk7AsIGy22Uo3fdq49Wm\n+uzfN3gRoWyuLAyfTqkO57D7O4JBzVw/EzXJZ26fwYGUZpciM1H4rhApde823jgwJprBvOQpF6aA\ncZZpl8GAhu0IwwXw7eOLB8J3l+8uMdT3RabgD/c/3Ms7ECMfjXwkRkCyPbDdorNFZ7F/Lo4uUgmI\nyViT8qVLjnxHbrxcBA+RvlM2p3DDxq+LRypKc6LmRKlN3z34MKTrE9bD0KYnpANNB5vihmxzuA1S\nXrn1ckFQojOi8YD6GvkaSTzukb640ZYWLAXhKoksQaRwT+c9nSVmQGoXofRqQsaEjGq77nWferA4\n3m8geVFfLzeCARQpfjvseZdkQMgbmq9gfKjw3lQhi3kDdm+t7P3c/UN3vK+oTFSGiP2ymcu4lPgM\n3oxvQUACTQKhX3LX5UJ/mF0yk9rU2KmsEwxO4fPC5+X15uff+1T62rhNY3zu+swVgQ+frT4IwITo\nhYBYxYfGgxh8CPwAx3rN0jWIZDHOjHP59eaazkWKPWdcDpo6rc9bSx024j3JGxnm0jal2H47r3Ze\nkghI3ug8GED/uf5Sp3817NwQ+ixZPhlEYH3Z+jKxHpX9TijZfb3pNQIrR2cdBQHy1fPVk6g3+/ui\nVGzd1nU4H2U7ymBfgnsE9xAjIMP7Dq8uAvLe8tpAFu2EvUgTSK8n9DmmWAl05HDwWg7PLuUwRJ/9\n84Khz1Vyv7WtKgF5F/gO9/fVS1ehh3wFvshg+PJ8eWIo9MV0q3mH5h1CJN0mBn5Qqk8qHOmBSwai\nZJLH58Ff4r/go5dt847NeM4K3QvhpwWPCkbgw3eB7wKx53OnLwiBr5svAh8XfS5i3eL4YgSM20S2\ngZ/jttMN3ysdWDqwfEXKF49XmYdMi6KJIvSuwhMF9JbU+a0OMjPXYq6h9Pa59XOUbs25OOeiRP2x\n3hf+7MzFMxEweqLzBJnoxEaJCOhqXtW8Wr5Z+38ISIxKTPn9GrV1FPRQqUopAkIDwwaKlcCZWpqC\nwCWPSsb5Wj1zNewSs5FBi8CYemPgL77r8Q76a/fZ3fBvfZv5NpO4/6180Yu49dDWQ+X95SnZU8Qq\nhiY6ToSeLupbBGLqEOsQ+58SEFl9WTj6jSIb4QZoaNQQjrGMUKZC49Zq1amFlJFnpCd+n1+aD6Ly\n58g/4UjXFtWG4Z+6bSqah95GvIWjH68TjwssWiRC6k20QbRBDOeIUAMo2irChTyvfx776fTc6bkY\nAcn3ACMt6lWETIbDfgepU67G2oy1ESMgjCMyNYwDgyYr0RsRMj4xejEwdKqZqpkVOQ9BzkEw8Bn2\nGSipMjU2hQKwb2APZp8nzMP5fLz2MSIGIh+Rj8TjXiLCgy7aJILhvSBzAaUFs0WzRZIISOHFQjxQ\nnrqeutV3IyWu4dAogtbpUvn3RGM5h2vIm5Fd67B/rp0pM570gDSq7P3c4lELBCyyhmfB8YxYHwFD\nI+8u716V58PFzgUlqduPbEdkyXaV7SqJU7CsclECZOpo6vg1BET5kjIch4B1ASA0KaYpIA5Z/bPg\ncD9JeIKAx/W51+Hwh38IB+Eq8ivyAzGxCbGRSEBazkWTZXZ8NhwA63bW7aqFgPTIg8FcuHLhyq8h\nIL079O5QfirjffP75lLtxe8i9LyIlopAuETPRTifnvM854kRkLF9x1ZjD4iAQ9W29NmlUgUh76Oa\nQF7oOJ8QWaVR3H01oOlX94CUvcdzFTomtFJN6OZLzOGHPBv0DJUtJ1uehL6QWysH/0XmsgxKk7a/\n3Y5hRSWaJZpwzKfEoQdDtFK0UuJzGiJCCZEoTASH/PjF4/BfLKZbYLiGRxsP6JfSG6XIwHTU6KhR\nleOvq1QXwz3iS+Phj2Y1zsIwElE/UT+J+7VWxPlloSIEcER3RMh87FqzC9O8VFupwsEX3BLckkhA\nzquIBXAaeTdCyVjSoCScv82vNqPUX2a6DI7T67UXenXz3PJAyBysHazFepuveSKDW1a3DL0tCcMT\nYL9EC0ULJe7/AtGC8v6yKE2ETNYAZoBYJdLEYRPRm1cwpgD3Q7t+7fr9pwSkVlQtjFk8tewUUtMn\nrp3Agav1UOtRmXXkneVh4KJbReNC5R7PxQufjG8bI0Myfut4nJiCeQUwCJ2VOuMGEdoL4bALRwlH\niWEnIVerpyqHZnRFdUWkomR2yOyQSEAGFoExO/zp8GelCIjAkatpdOPhRjjoehA31P1h93GhtMdq\nj5VK4Mxk4XBst9sOxyR9bjocAdMlpniArZKtUEKWk5wDnHd+Hm5UYW1hbYnHPVqICKJQQYhUoEKa\nAm4kYR9hH4kE5HIhFIGnh6cHfQ8Ile9bnIZzOGUCh8x+7j7kXyMEJKuy97O6hbpF+abBZL9kOOaN\nHzau1HAGRsQgABA2LQzTq96UvoFh67KoyyKJBMQ+lwtEeJt6V4qAWHuLGSLXGFdE1kpSS9DTFv5X\nOPRWl8NdkFn99IKwWnNqwVHXD9ZHiWza+TToh7XOa50lEpAWc2FAsx9lg6BZ97HuUy0EpFceAkEL\nDyw88DUExDHSEQGtEqUS2AmvUi+cb0GuIFei3hwqRO25cLEQEUyFPAUMHREMEYiVbvVd03cNnYJF\npWZIfVJ69TvpHaozuvynHz+6Naraffn1U7AUVBXgd0VZRSGQctPpJnrJNOZqwL/hy/IxNGPj+Y3Q\nM0kNkhBobdyvMRxa4RDhEInPKV8IfSWXLIf9UjiqgFJL5h6DHgi3N24IAJc+LsUUU9eBrgOl7q8v\nD03ZRu5GCCiZuZsBtXZroSTsqvZVVNycvXoWGQwVPRU9iftFUHBSgF4I+bfyIFbyi+W5EtqjPOyn\n8ILwQkUIiGyxLF6HsGPzDkzD+lvNIhNhKmOKgHJkVCT87kttL2Gcr2qcapxYb+CD4ZiiWFpYimb+\nX4x/wfUUWggtJO4/8Q+Fx4XwuxWbK6J0TtZFVqwiaOLQidCTBbMLUOrb7kC7A/8pAZHxkUEkfu/U\nvaily2FywJjsTtudrsw6dWbXwQEljE9Aze6D7Q9g8Osq1kXzusNuB9wQxQrFcKz9xvuNr8i6Qy4P\ngYO9+ulq9EToXNG5Uq0ERNVRtfznM4/NRC9J8dFi3HA9DXsaSltPb4Iemozu6N5BBuLFlRfYP3Mt\nc0zR0szSRJPP3Zd3MTYtSi0KTVeKrxRfSe0BMbOGIxPmH4aSt/bu7d0pAaHyY0stTxIBlBU3xDJd\nCAGp/HtAgniorZ20axJS1WXqZQhkBF8JxnMqaCFoUZF1Ggc3hmOfODcRBvim1U0YZq1srexqJSB/\n/6D858EXgrk5/CtyMFSiafOmzaWtZ3bPDIY8yyALzfAhq0PQNMlYMVbfEwFp2LghIpbJ/snQfzuO\n7EDGSWApsJS2bufmnXF+QvuFwhGy7GrZlY7hpVIzRRjKoXqepE+r5T0gVSQgcpFyCAAcERxBgPZ+\nyn0MrdDN0wWx5zE8+Inj14wHoS96XATC0Kd+n/pS3zfRSwD9ML3hdPTAztOch8yJwhUF6ONm4c0w\nRehV6CtkIpbrLOeGcuzk7ZTYA1JbGYHcY0nH0GMRqR0JwqF6WRV+0Za/BXokJhmBHLNCs0KpfmxU\nHRCD5Y7Loc/GNB8DfcJfzYceFQYIAypCQD7hIKtB0Lt5e/P2YnqXziIcT1JxEgjKlPgpEnuWWz1r\nhRLb3DO5eM/H4qzF3DSuZbxl0va/T3Yf2KN1Z9ehZMs43Ti9RhOQT+g02gmR91eFr3CB4mTjwHA7\nduiIVLhGngZuPPlAeShs5Uhl3KBGzY1wgYIGB6Hn4V2td7VgiDsFIoPB38wHA1RfqY6U3LnN5/D3\nlIcpD8szO9VEVTRJqcSpgAk6mznDkKfkpWC7p8NP48ZU9VD1qFYCouWoJfaejlJLRNpS41NxY8TG\nxOLGtVK3guOiFKCEG1D3ii4emGXrlq0r38yZ7JUMw2xhY8GVPuQz2L+F0QvRC1MiW4Lz6tfTD01f\nde3qInOieFMRzZjmOeZopjrlego3dsaADMz7t3KzcqMEhMrPKNXxJnRtB22UWJ4pO8M5/K0LW2PK\nSdJ8GC7jB8aIOCnOUsRzrBKrgtrYlqtaosQq4k0EInOvW75GKYLncU9EmhgdRqdaCUhT76blP58x\nfwamOBVdKYK+6f1bb9Q28y/z8dzzV/BBTBqmN4Sh2dZ7G5q5P4g+IGOz4coG/I7RZrS/JwLyKdOx\nPnY9rkP+83yupErWE/pTc4Qmxl4qtVNC6Zh1HWuUAl8ZcAX68uH1h2huNR5gPIASECrfp977DwkI\nTw542OUw99qAlanw30zSTNLEAjOTGkMvPN76GBUuVw9fRWa2jWsb+C/Kk5Uxdlx9hjp67sYUjoF/\nWahYiMD06rLVeO5lp8oiAK6wUAElRnuj9oIIZIRnwO/rd6rfKWR6B9VCSZNqiioI0bh+4xBoKHEs\ngf4JNAxEwJjpxUAPuUS7wO8qSCqAnt/dcjf0t8lKk5Xl31+i+7fAj50RhP18ve41/Lpf3//KjRuf\nxZtVPsNQUQJiYGKA3pR7+ffgB77Mewl/9umrpwhAm2803yiRWI1TRm/e4d6Hoc8zCzLREzxw/EAE\n7tV2qyGgr3xFGfq9o05H2KGH4Q9xvq4euopekLpudd2+CwIiayMLh3lE5gj0PDx9/BSMNvtZNpjY\nlfQr6eVr9s65ncOBPb78GIYwv0k+5hRvV9yOG0srRytH0nbar24PJnn3+V0YlNzCXNyQl6Iu4YaL\nXh2Nz7MistArcm/UPaSY7C/bX5a0nkcLD9wIBSsKYIgd+jhINaQefh4owShULUTmo8vaLmvFMkLd\nZbqjqXvmBDQFZe/PRk9J8uxkXLAzq89g/xLOJoBhPqv3DM1ChzsfhkF9FPIIBs5iqYXYGDPds7r4\nfkTnCHyvMK0QD/JNuZsYG3fa5DRu1MQNiajpezniJQzs+KDxiODKHJc5Lua4OHSCQ1XAK8Dfx20f\nt50SECqUgPzDe0GuNUGJ6WnV03j+S+VL4YgmNkxERO6U7ymk9KP3R+O5T3ufBgOUXS8bz3lgRiCG\nPSjvVt4tcQqWZQAi9NlzslESZWpoKjWD2mlBJ9TuFugUwIB4TfKaVP7zZt2aYYre/Yf3EbBJaZcC\nh3v/gv0LyhvqG043UCIRYRIBPZIiSEHk8kXIC+ijyaMnjxabguU4BwY7Sz8LvXVWuVa5Uqdg7fHC\ni1eLPYsx396+1L5ULPNSYobx5jnLchChW3BiwQmp8+8/GKNX5fmo59Dv656vey7peyadTaAvz10+\nB/1f6FAIvXc17Coch9MPT3PnpXUKCGVKXAoCWO5+7tDzTEemo1iE8G0frmbdpgT2rn9K/xRKQKj8\naATE7qgdKjjK5Mug3zZ23ti5UiWnIxn08K56vgrP5euE1+g1817vjR46/q98TMNjGjLQm+5N3OH/\npW5ORYA5bUsaMg/nnM5BL8WejkVFTb5yvnJ5/9HkickTidP8FrYEEbnZ4CZKu3IicuAPivaKkEm4\ndOASHOe8jXlw4KPVolFZYpRqJPb6BbmRcjiOAMMA6OHc1Fx8/njeY7QCnLp7CtMHE64lwC4UtiiE\nP7lx80Ych9oFNbFxvcKLQvi/MdExIDbncs9Bb6rsUtklcUhREh/EZ43rGhCVjx0+IqC/w28H9JOg\nraCttOtgFWOFAHh8UDz8wDzfPNinWO1YBJTOXTyH/ckIyEBg/HHPxwhsO/Vz4no7hvLE3lM1YcUE\n+Ml5J/Kgn9u1bde2Zk3BKuSBEFgUWgB/U/8Nkf8dH3bAYJwwP4FmwPAj4UiJL7VaihST0xMn3EjK\n+5X3V2Q7pjqmMLizd85Gai3CPQIlRoeXHYYBm2s2F5FEcxNzE2nr6Ofpg1E6TXTCtKraI2tLHVdm\nIDCAYXZa6wTiUftSbYlTamTXyuJzR0VHEKoQpxA8SFGOUTDc6zLWwRHpcLcDbmCDJwY4/s69OoN5\nq1irWEtat3aD2nigRpeOhgHffWw3Sr6OnzoOhr88fjkyL532dsKDJttLtpekdTQvauLGc3JwgkE2\nCDcIpwSECiUgFSwZja4DAzIkcwgCLqFTQtE8eZx3HJ8fMTkCvfP74d8R0Wuf3B6RROEc4Rypek1g\nCv3S7UU3TIlRbqQsdSy25l1N6A8nZyf0aBi2NRQ3CAG8gPLjdYPuBKE5Mqo4Cin87XrbUdPs5uGG\nDKj6MHX0rg35MAT6+lCLQzCoU/ZM2SPm2GeZIKXfbVY3RPZUT6mekrafhk6G0H/OLs6IiGpM05gm\nFrE7pgw91r1td+x/Y8fGUpvvFV8o4vw4ZDugZKBpeFOp+kvnFx28uNE73xuRxP299yMyeNT9KOzG\nkj+W4I3AbTPaQi/z9/L3SsyE3dJGE6nzUmcEiHR5ujxKQKj8aAREQ1MDpU09/hYEMo42O1qVdYzC\njED0e9r2xFja1g9bg/Dz3/HfSeqRa6PUBj1bSyyXIBBz7P4xvD5gb529yFBOrD0RJVO6YboVejGi\n0SUj+GezDs7Ce4UOPTqEjO2h+4ewrl9bP+gb/R36O6T2KDeShx528XFBy8H6mPVw7KPuRaFkdYv7\nFuiRQTmDEDiv1aRWE4mEQomP47NbaYcMis18G2SoZRvINpC2/QauDUBAeuT2AGEx0TOp1Dj4Bt4N\nkEn37eiLgEr4y3CU9B85eATnZYHVAvjhzZKaceN++/L6SlzHogF6SLo/647EgsZijcU1egzv/+EB\nHhgn85RBLwbTk+lZneszsxlkGJjxzPhvsv/VtZ93mbvVuu4jHh4o5hpzreYcJyUgVH5sAvI/aMxD\nyQJznkEqnVnELKrReug2g+EevKm8qVK/58sgYsbz5/nX5OOp9PEbMlypxW5m93+3H5SAUKmZBKTG\nPKfkNQy8t7y31bLeKAYZU2YMM+ar1pPjcS8sfMJwGZixvLHf1XkdwKC0lAlmgmvOfn1LAkLxJ0JK\nQKj8ZASEIkVKQKhQAkKRIiUgFCkBoUKFEhCKlIBQoQSEIkVKQChSAkKFCiUgFClSAkKFEhCKFCkB\noUgJCBVKQChSpASEyg9CQDzpc0bx+8E0F0pAKFICQoUSEIoUKQGh8l3rvd71SWQ5n+CfFCnWPExT\n4TA+7f8B+P15KNMCkjoAAAAldEVYdGRhdGU6Y3JlYXRlADIwMTgtMTAtMThUMTI6MDM6NTkrMDA6\nMDCZ9LIMAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDE4LTEwLTE4VDEyOjAzOjU5KzAwOjAw6KkKsAAA\nABR0RVh0cGRmOlZlcnNpb24AUERGLTEuNSAFXAs5AAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%tikz\n",
    "\\tikzset{every node/.style={font=\\sffamily,white}} \n",
    "\\node[fill=blue] at (0,0) (a) {Sender}; \n",
    "\\node[fill=blue] at (3,0) (b) {Channel}; \n",
    "\\node[fill=blue] at (6,0) (c) {Receiver}; \n",
    "\\draw[->] (a) -- (b) node [midway,above,font=\\scriptsize,black]{$p(\\mathbf{t})$}; \n",
    "\\draw[->] (b) -- (c) node [midway,above,font=\\scriptsize,black]{$p(\\mathbf{s}|\\mathbf{t})$};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A message $\\target$ is sent through a noisy channel and $\\source$ is received\n",
    "* What was $\\target$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Channel could be \n",
    "\n",
    "* a ethernet cable\n",
    "* an encryption algorithm\n",
    "* a speaker that thinks in English but speaks in German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Noisy Channel  in SMT\n",
    "\n",
    "* We are interested on modeling the best model that fits the data. Best model t^ is selected by considering how well it explains the date p(s|t) and how good it flows in general p(t)\n",
    "\n",
    "* Bayes’ theorem then shows that the posterior probabilities are proportional to the numerator: t^ is the point estimate of the posterior\n",
    "\n",
    "\n",
    "$\\prob(\\target,\\source) = \\prob(\\target) \\prob(\\source|\\target)$ is often called a **noisy channel model**\n",
    "\n",
    "<center><img src=\"../chapters/mt_figures/noisy_channel.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Noisy Channel in MT\n",
    "$$\n",
    "\\prob_\\params(\\target,\\source) = \\prob_{\\params_t}(\\target) \\prob_{\\params_s}(\\source|\\target)\n",
    "$$\n",
    "\n",
    "* $\\prob_{\\params_s}(\\source|\\target)$ is called the **translation model**\n",
    "    * Does source match the target?\n",
    "* $\\prob_{\\params_t}(\\target)$ is the **language model**\n",
    "    * Does target look like a real language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MLE for $\\prob_\\params(\\target,\\source) = \\prob_{\\params_t}(\\target) \\prob_{\\params_s}(\\source|\\target)$ can be calculated in two **independent steps**:\n",
    "* Estimate $\\params_t$ for $\\prob_{\\params_t}(\\target)$\n",
    "* Estimate $\\params_s$ for $\\prob_{\\params_s}(\\source|\\target)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What training data do you need for each? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Translation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\prob^{\\text{Impossible}}_\\params(\\source|\\target) = \\params_{\\source,\\target}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example:\n",
    "$$\n",
    "\\prob^{\\text{Impossible}}_\\params(\\text{Ich mag Musik }|\\text{ I like music})=\\params_{\\text{Ich mag Musik},\\text{I like music}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why impossible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generally we want models that **factorize** (break up into smaller parts) for dealing with \n",
    "\n",
    "* **data sparsity**\n",
    "* **memory limitations**\n",
    "* **runtime limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How did **language models** do this? \n",
    "\n",
    "$$\n",
    "\\prob(\\text{I like Music})=\\prob(\\text{I}) \\prob(\\text{like | I}) \\prob(\\text{Music | like})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Look at example for inspiration:\n",
    "\n",
    "|Token| 1 | 2 | 3 | 4 | \n",
    "|-|---|---|---|---|\n",
    "|**Target**| the | house | is | small |\n",
    "|**Source**| das | Haus | ist | klein |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\prob_\\theta(\\source|\\target) = p(\\text{das | the}) * p(\\text{ Haus | house}) \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Naive Translation Model\n",
    "\n",
    "$$\n",
    "\\prob_\\params^\\text{Naive}(\\source|\\target) = \\prod_i^{\\length{\\source}} \\prob_\\params(\\ssource_i|\\starget_i) = \\prod_i^{\\length{\\source}} \\param_{\\ssource_i,\\starget_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why naive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "Use the **Maximum Likelihood Estimate**:\n",
    "\n",
    "$$\n",
    "\\params^* = \\argmax_\\params \\sum_{(\\target,\\source) \\in \\train} \\log \\prob_\\params(\\source|\\target)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Amounts to **counting**: \n",
    "\n",
    "$$\n",
    "\\param^*_{\\ssource,\\starget} = \\frac{\\counts{\\train}{s,t}}{\\counts{\\train}{t}} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "def learn_naive_model(data):\n",
    "    norm = defaultdict(float)\n",
    "    counts = defaultdict(float) \n",
    "    for target, source in data:\n",
    "        for i in range(0, len(target)):\n",
    "            norm[target[i]] += 1.0\n",
    "            counts[(source[i],target[i])] += 1.0\n",
    "    result = {}\n",
    "    for (source,target),score in counts.items():\n",
    "        result[(source,target)] = score / norm[target]\n",
    "    return result\n",
    "# show defaultdict behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\" style=\"font-size:2em;\"><tr><td style='padding:20px; text-align:left'>('das', 'the')</td> <td style='padding:20px; text-align:left'>1.00</td><tr><tr><td style='padding:20px; text-align:left'>('Haus', 'house')</td> <td style='padding:20px; text-align:left'>0.50</td><tr><tr><td style='padding:20px; text-align:left'>('Gebauede', 'house')</td> <td style='padding:20px; text-align:left'>0.50</td><tr></table>"
      ],
      "text/plain": [
       "<statnlpbook.util.Table at 0x7ff080983f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_model = learn_naive_model([[('the','house'),('das','Haus')],\n",
    "                                 [('the','house'),('das','Gebauede')]])\n",
    "# try other genders or numbers \n",
    "util.Table(naive_model.items(), \"2em\", \"20px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"left\" style=\"font-size:1.5em;\"><tr><td style='padding:20px; text-align:left'>['the', 'house', 'is', 'small']</td> <td style='padding:20px; text-align:left'>['das', 'Haus', 'ist', 'klein']</td><tr><tr><td style='padding:20px; text-align:left'>['the', 'house', 'is', 'small']</td> <td style='padding:20px; text-align:left'>['klein', 'ist', 'das', 'Haus']</td><tr><tr><td style='padding:20px; text-align:left'>['a', 'man', 'is', 'tall']</td> <td style='padding:20px; text-align:left'>['ein', 'Mann', 'ist', 'groß']</td><tr><tr><td style='padding:20px; text-align:left'>['my', 'house', 'is', 'small']</td> <td style='padding:20px; text-align:left'>['klein', 'ist', 'mein', 'Haus']</td><tr></table>"
      ],
      "text/plain": [
       "<statnlpbook.util.Table at 0x7ff082ab8f28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.Table(train, \"1.5em\", \"20px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAGPCAYAAACefJ5lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff082ab89e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = learn_naive_model(train)\n",
    "plot_table_for_target(table, \"is\") # try house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Translation / Decoding\n",
    "\n",
    "Find the highest scoring target $\\target$ \n",
    "\n",
    "$$\n",
    "\\target^*(\\source) = \\argmax_\\target \\prod_i^{\\length{\\source}} \\prob_\\params(\\ssource_i|\\starget_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How about **brute-force** (like in our Structured Prediction example)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Local Decoding \n",
    "\n",
    "How to find this $\\argmax$?\n",
    "\n",
    "$$\n",
    "\\argmax_{y_1,y_2} f_1(y_1) f_2(y_2) = \\ldots\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Local Decoding for the Naive Model\n",
    "\n",
    "$$\n",
    "\\target^*(\\source) = \\argmax_\\target \\prod_i^{\\length{\\source}} \\prob_\\params(\\ssource_i|\\starget_i) \\\\\n",
    "= (\\argmax_{\\starget_1}  \\prob_\\params(\\ssource_1|\\starget_1), \\ldots, \\argmax_{\\starget_{\\length{\\source}}}  \\prob_\\params(\\ssource_{\\length{\\source}}|\\starget_{\\length{\\source}}))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UniformLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8cb2c85a325c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniformLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ein\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mann\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# try other inputs with given vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UniformLM' is not defined"
     ]
    }
   ],
   "source": [
    "def decode(source_sent, model, lm):\n",
    "    source_to_targets = defaultdict(list)\n",
    "    for (source,target),prob in model.items():\n",
    "        source_to_targets[source] += [(target,prob)]\n",
    "    result = []\n",
    "    for tok in source_sent:\n",
    "        candidates = source_to_targets[tok]\n",
    "        multiplied_with_lm = [(target,prob * lm.probability(target)) \n",
    "                              for target, prob in candidates]\n",
    "        target = max(multiplied_with_lm, key=lambda t: t[1])\n",
    "        result.append(target[0])\n",
    "    return result\n",
    "\n",
    "lm = UniformLM(set([target for _, target in table.keys()]))\n",
    "decode([\"ein\", \"Mann\"], table, lm) # try other inputs with given vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alignments\n",
    "\n",
    "<center><img src=\"../chapters/mt_figures/alignment.png\"></center>\n",
    "\n",
    "Naive Model assumes a sequential **alignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mt.Alignment(\"the house is small\".split(\" \"),\n",
    "                  \"das Haus ist klein\".split(\" \"),\n",
    "                  [(0,0),(1,1),(2,2),(3,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But word order can differ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_mt.Alignment(\"the house is small\".split(\" \"),\n",
    "                  \"klein ist das Haus\".split(\" \"),\n",
    "                  [(0,2),(1,3),(2,1),(3,0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formalise alignments:\n",
    "\n",
    "* for each source sentence index \\\\(i \\in [1 \\ldots \\length{\\source}]\\\\)\n",
    "    * \\\\(a_i \\in [0 \\ldots \\length{\\target}]\\\\): index of **aligned target word**\n",
    "* \\\\(\\align_i\\\\) can be \\\\(0\\\\)\n",
    "    * Corresponds to imaginary _NULL_ token \\\\(\\starget_0\\\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "word_mt.Alignment(\"NULL the house is small\".split(\" \"),\n",
    "                  \"klein ist das Haus\".split(\" \"),\n",
    "                  [(1,2),(2,3),(3,1),(4,0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$a_1 = 4$ ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why the **NULL** Token? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_mt.Alignment(\"NULL I like music\".split(\" \"),\n",
    "                  \"音楽 が 好き\".split(\" \"),\n",
    "                  [(0,1),(2,2),(3,0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One-to-many translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word_mt.Alignment(\"NULL the house is tiny\".split(\" \"),\n",
    "                  \"das Haus ist sehr klein\".split(\" \"),\n",
    "                  [(1,0),(2,1),(3,2),(4,3),(4,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Many-to-one translations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alignments are **hidden** when given a source! \n",
    "\n",
    "Need to be \n",
    "* predicted or marginalised out\n",
    "* modelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IBM Translation Models\n",
    "\n",
    "In the late 80s, early 90s\n",
    "* **IBM researchers revolutionised MT** (and Speech Recognition) \n",
    "* using statistical approaches instead of rules \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A famous IBM quote from that time\n",
    "\n",
    "> Every time I fire a linguist, the performance of the speech recognizer goes up\n",
    "> \n",
    ">  -- <cite>Fred Jelinek</cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Group around **Robert Mercer** were extremely successful\n",
    "\n",
    "* then disappeared to create **Renaissance Technologies** \n",
    "![mercer](http://images.huffingtonpost.com/2014-10-23-RobertMercer-thumb.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IBM Model 2\n",
    "Translation and **alignment** model:\n",
    "\n",
    "$$\n",
    "p_\\params^\\text{IBM2}(\\source,\\aligns|\\target)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Generative story* with parameters $\\params = (\\alpha, \\beta)$\n",
    "\n",
    "Given target sentence $\\target$ (e.g. NULL I like music) with length $l_\\starget$ (e.g. 4):\n",
    "\n",
    "* Generate a **source sentence length** $l\n",
    "_\\ssource$ with uniform probability $\\epsilon$\n",
    "    * e.g. $l_\\ssource=3$\n",
    "* Generate a **target position** $a_i$ for each source as position $i$ with $\\beta(a_i|i,l_\\starget,l_\\ssource)$\n",
    "    * e.g. $(3,0,2)$\n",
    "* Generate the **source word** $\\ssource_{i}$ at $a_i$ with probability $\\alpha(\\ssource_i|\\starget_{a_i})$ \n",
    "    * e.g. (音楽, が, 好き)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "  p_\\params^\\text{IBM2}(\\ssource_1 \\ldots \\ssource_{l_\\ssource},\\align_1 \\ldots \\align_{l_\\ssource}|\\starget_1 \\ldots \\starget_{l_\\starget}) = \\epsilon \\prod_i^{l_\\ssource} \\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example:\n",
    "$$\n",
    "p_\\params^\\text{IBM2}(\\text{das, Haus},1, 2 | \\text{NULL, the, house}) = \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Answer: \n",
    "$$\n",
    "\\a(das|the) \\b(1|1,3,2) \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training IBM Model 2\n",
    "\n",
    "If we had $\\train = ((\\source_i, \\aligns_i, \\target_i))_i$ we could optimise\n",
    "\n",
    "$$\n",
    "\\sum_{(\\source, \\aligns, \\target) \\in \\train} \\log p_\\params^\\text{IBM2}(\\source,\\aligns|\\target)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But we have **no training alignments** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead use **sentence-aligned** data $\\train = ((\\source_i, \\target_i))_i$ \n",
    "\n",
    "and **marginal log-likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "\\sum_{(\\source, \\target) \\in \\train}   \\log p_\\params^\\text{IBM2}(\\source|\\target)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "p_\\params^\\text{IBM2}(\\source|\\target) = \\sum_{\\aligns}p_\\params^\\text{IBM2}(\\source,\\aligns|\\target)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**No closed-form** solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectation Maximisation (EM) Algorithm\n",
    "maximises a [lower bound of marginal log-likelihood](http://localhost:8888/notebooks/chapters/em.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Figures out (soft) alignments by iterating:\n",
    "\n",
    "* **E-step**: Infer alignments (or their **expectations**)\n",
    "    * using current parameters\n",
    "*  **M-step**: **Maximise** training objective to estimate parameters \n",
    "    * using current alignments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How is this possible **without good initial parameters**? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you find out which target words belong to which source words? \n",
    "\n",
    "Assume static word order (but possibly different in each language)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word_mt.Alignment(\"Bb Aa | Aa Cc | Qq Pp\".split(\" \"),\n",
    "                  \"Xx Yy | Zz Xx | Ss Tt\".split(\" \"),[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\a(X|A)= 1/2 \\qquad \\a(Y|A)= 1/4 \\qquad \\a(X|B)= \\qquad \\a(Y|B)= \\qquad \\a(T|P)=$\n",
    "\n",
    "$\\b(1|1)= 1/2 \\qquad \\b(1|2)= 1/2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formalise and implement, but for dataset with `NULL` tokens and \n",
    "\n",
    "* **non-monotonic** and **consistent alignment**\n",
    "\n",
    "such that EM will \n",
    "* first learn about word-to-word correspondences\n",
    "* and then about distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_model_2_raw = [\n",
    "  (\"NULL the house is small\" , \"klein ist das Haus\"),\n",
    "  (\"NULL a man is tall\" , \"groß ist ein Mann\"),\n",
    "  (\"NULL my house is small\" , \"klein ist mein Haus\"),\n",
    "  (\"NULL the building is big\" , \"groß ist das Gebäude\"),\n",
    "  (\"NULL the building is long\" , \"lang ist das Gebäude\")\n",
    "]\n",
    "train_model_2 =  [(t.split(\" \"), s.split(\" \")) for t,s in train_model_2_raw]\n",
    "# What could \"house\" translate to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "util.Table(train_model_2_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### E-Step\n",
    "\n",
    "Calculate posterior distribution over alignments given source and target \n",
    "\n",
    "$$\n",
    "\\pi(\\aligns|\\source,\\target) = p_\\params^\\text{IBM2}(\\aligns|\\source,\\target)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distribution **factorizes** for Model 2:\n",
    "$$\n",
    "\\pi(\\aligns|\\source,\\target) = \\prod_i^{l_{\\ssource}} \\pi(a_i|\\source,\\target,i) \n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\pi(a_i|\\source,\\target,i) = \n",
    "  \\frac\n",
    "    {\\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)}\n",
    "    {\\sum_j^{l_{\\starget}} \\alpha(\\ssource_i|\\starget_j) \\beta(j|i,l_\\starget,l_\\ssource) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "word_mt.Alignment(\"1:NULL 2:the 3:house\".split(\" \"),\n",
    "                  \"1:das 2:Haus\".split(\" \"),[(1,0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\pi(2|\\source,\\target,1) = \\frac{\\a(\\text{das|the}) \\b(2|1,3,2)}{\\a(\\text{das|the}) \\b(2|1,3,2) + \\a(\\text{das|NULL}) \\b(1|1,3,2) + \\ldots}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a uniform initial model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "source_vocab = set([tok for _,s in train_model_2 for tok in s])\n",
    "target_vocab = set([tok for t,_ in train_model_2 for tok in t])\n",
    "\n",
    "alpha = mt.create_translation_table(source_vocab, target_vocab) \n",
    "#  {'is':{'ist':0.5, 'das':0.5}}\n",
    "beta = mt.create_distortion_table(5) #{1:1}\n",
    "init_model = IBMModel2(alpha,beta)\n",
    "align_matrices = e_step(init_model, train_model_2)\n",
    "\n",
    "def show_alignment_for_model(sent, fixed_alpha = {}, fixed_beta = {}):\n",
    "    alpha_1 = mt.create_translation_table(source_vocab, target_vocab, fixed=fixed_alpha) \n",
    "    #  {'is':{'ist':0.5, 'das':0.5}}\n",
    "    beta_1 = mt.create_distortion_table(5) #{1:1}\n",
    "    init_model_1 = IBMModel2(alpha_1,beta_1)\n",
    "    align_matrices = e_step(init_model_1, train_model_2)\n",
    "    return word_mt.Alignment.from_matrix(align_matrices[sent], \n",
    "                              train_model_2[sent][1], \n",
    "                              train_model_2[sent][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# try  {'is':{'ist':0.5, 'Haus':0.5}}\n",
    "show_alignment_for_model(3,{'is':{'ist':0.5, 'Haus':0.5}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### M-Step\n",
    "\n",
    "The M-Step optimizes a *weighted* or *expected* version of log-likelihood using distribution \\\\(\\pi\\\\) from last E-Step:\n",
    "\n",
    "$$\n",
    "  \\params^* = \\argmax_\\params \\sum_{(\\target,\\source) \\in \\train} \\sum_\\aligns \\pi(\\aligns|\\target,\\source) \\log \\prob _\\params^\\text{IBM2}(\\source,\\aligns|\\target)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because \\\\(\\pi\\\\) factorizes we have a closed-form solution:\n",
    "\n",
    "$$\n",
    "  \\alpha(\\ssource|\\starget) = \\frac\n",
    "    {\\sum_{(\\target,\\source)}\\sum_i^{l_\\source} \\sum_j^{l_\\target} \\pi(j|i) \\delta(\\ssource,\\ssource_i) \\delta(\\starget,\\starget_j) }\n",
    "    {\\sum_{(\\target,\\source)} \\sum_j^{l_\\target} \\delta(\\starget,\\starget_j) }\n",
    "$$\n",
    "\n",
    "\\\\(\\delta(x,y)\\\\) is 1 if \\\\(x=y\\\\) and 0 otherwise\n",
    "\n",
    "$\\beta$ parameters estimated similarly ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show_alignment_for_model(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "  \\alpha(\\text{das|the}) = \\frac\n",
    "    {\\sum_{(\\target,\\source)}\\sum_i^{l_\\source} \\sum_j^{l_\\target} \\pi(j|i) \\delta(\\text{das},\\ssource_i) \\delta(\\text{the},\\starget_j) }\n",
    "    {\\sum_{(\\target,\\source)} \\sum_j^{l_\\target} \\delta(\\text{the},\\starget_j) } \\\\ = \\frac{0.2 * 0 * 0 + \\ldots + 0.2 * 1 * 1 + \\ldots}{0 + 1 + 0 + 0 + 0} = \\frac{0.2}{1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Implement M-Step, estimating parameters \\\\(\\params\\\\) from (soft) alignments \\\\(\\aligns\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "theta1 = m_step(align_matrices, train_model_2)    \n",
    "def show_initial_alpha(target):\n",
    "    plot_table_for_target(theta1.alpha, target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show_initial_alpha(\"is\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "align_matrices = e_step(theta1, train_model_2)\n",
    "def show_initial_alignment(sent):\n",
    "    return word_mt.Alignment.from_matrix(align_matrices[sent], \n",
    "                                  train_model_2[sent][1], \n",
    "                                  train_model_2[sent][0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do alignments look like with these parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show_initial_alignment(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Initialisation\n",
    "Good initialisation is **crucial for EM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baby Steps\n",
    "Initialise with the parameters of a simpler model with **fixed distortion** table:\n",
    "\n",
    "$$\n",
    "  \\beta(a_i|i,l_\\starget,l_\\ssource) = \\frac{1}{l_\\starget + 1}\n",
    "$$\n",
    "\n",
    "This is **IBM Model 1**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Train IBM Model 1 and see if it **converges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ibm1_iterations = em_model1(init_model, train_model_2, 100)\n",
    "def plot_ibm1_change(end):\n",
    "    plt.plot(range(0,end), [change for _, _, change in ibm1_iterations[:end]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_ibm1_change(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Translation Table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_ibm1_alpha(target):\n",
    "    plot_table_for_target(ibm1_iterations[-1][1].alpha, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show_ibm1_alpha(\"house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many-to-many alignments?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_ibm1_alignments(sent):\n",
    "    return word_mt.Alignment.from_matrix(ibm1_iterations[-1][0][sent],train_model_2[sent][1], train_model_2[sent][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show_ibm1_alignments(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distortions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_ibm1_distortions(si):\n",
    "    util.plot_bar_graph([ibm1.beta[ti,si,5,4] for ti in range(0,5)],\n",
    "                        range(0,5)) # change source index != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show_ibm1_distortions(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now learn **distortion table** in Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ibm1 = ibm1_iterations[-1][1] # model 1 of last iteration \n",
    "\n",
    "def show_ibm2_alpha(num_iterations, target):\n",
    "    ibm2_iterations = em_model2(ibm1, train_model_2, num_iterations)\n",
    "    ibm2 = ibm2_iterations[-1][1] # model 2 of last iteration\n",
    "    plot_table_for_target(ibm2.alpha, target)\n",
    "    \n",
    "def show_ibm2_alignments(num_iterations, sent):\n",
    "    ibm2_iterations = em_model2(ibm1, train_model_2, num_iterations)\n",
    "    ibm2 = ibm2_iterations[-1][1] # model 2 of last iteration\n",
    "    return word_mt.Alignment.from_matrix(ibm2_iterations[-1][0][sent],train_model_2[sent][1], train_model_2[sent][0])\n",
    "\n",
    "def show_ibm2_distortions(num_iterations, si):\n",
    "    ibm2_iterations = em_model2(ibm1, train_model_2, num_iterations)\n",
    "    ibm2 = ibm2_iterations[-1][1] # model 2 of last iteration\n",
    "    util.plot_bar_graph([ibm2.beta[ti,si,5,4] for ti in range(0,5)],\n",
    "                    range(0,5)) # change source index != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show_ibm2_distortions(10,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many-to-many alignments? \n",
    "\n",
    "Run IBM models in both sides \n",
    "<center><img src=\"../chapters/mt_figures/ibm1.png\"></center>\n",
    "\n",
    "Then use heuristics such as union to combine the two uni-directional alignments:\n",
    "\n",
    "<center><img src=\"../chapters/mt_figures/union.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show_ibm2_alignments(10, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Translation Table? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show_ibm2_alpha(4, \"house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoding with Model 2\n",
    "\n",
    "Ideally **marginalise out alignments**\n",
    "\n",
    "$$\n",
    "\\argmax_{\\target} p_\\params^\\text{IBM2}(\\source|\\target) \\prob^\\text{LM}(\\target)\n",
    "$$\n",
    "\n",
    "with \n",
    "$$\n",
    "  p_\\params^\\text{IBM2}(\\source|\\target) =  \\sum_{\\aligns}  p_\\params^\\text{IBM2}(\\source,\\aligns|\\target) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computationally **very expensive**, instead find both optimal alignment and translation:\n",
    "\n",
    "$$\n",
    "\\argmax_{\\target,\\aligns} p_\\params^\\text{IBM2}(\\source,\\aligns|\\target) \\prob^\\text{LM}(\\target)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Choices:\n",
    "\n",
    "* Simple **approximate** method\n",
    "* Complex **exact** method\n",
    "\n",
    "Both presentend in [these slides](https://www.dropbox.com/s/p495n19h5rtk3uf/IBM-decoding.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beam Based Decoder\n",
    "* In each step chooses **best next source word to translate**\n",
    "* **Append a target word** based on source word\n",
    "* Maintains a list of top-$k$ hypotheses in a **beam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "uni_lm = UniformLM({w for w in target_vocab if w != 'NULL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_decode_history(begin, end, beam=4, lm=uni_lm, num_iterations=3):\n",
    "    ibm2_iterations = em_model2(ibm1, train_model_2, num_iterations)\n",
    "    ibm2 = ibm2_iterations[-1][1] # model 2 of last iteration\n",
    "    hist2 = decode_model_2(ibm2, lm, source, beam)\n",
    "    return mt.render_history(hist2[begin:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ngram_lm = LaplaceLM(NGramLM(lm_train, 3),0.1)\n",
    "show_decode_history(begin=0,end=5,beam=2, lm=ngram_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "* MT is an instance structured prediction recipe\n",
    "* The noisy channel is one modeling framework\n",
    "* word-based MT is foundation and blue print for more complex models\n",
    "* Training with EM\n",
    "* NLP Tricks: \n",
    "    * introducing latent alignment variables to simplify problem\n",
    "    * decoding with Beams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background Material\n",
    "* [Lecture notes on IBM Model 1 and 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf) of Mike Collins.  \n",
    "* Jurafsky & Martin, Speech and Language Processing: \n",
    "    * Chapter 26, Machine Translation.\n",
    "    * Chapter 6, EM Algorithm\n",
    "* Brown et al., [The Mathematics of Statistical Machine Translation: Parameter Estimation](http://www.aclweb.org/anthology/J93-2003)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
