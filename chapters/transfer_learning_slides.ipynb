{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Inference & Transfer Learning\n",
    "\n",
    "Daniel Hershcovich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "- Natural language inference\n",
    "- Quiz results\n",
    "- Multi-task learning\n",
    "- Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recognizing Textual Entailment = Natural Language Inference\n",
    "\n",
    "Determining the logical relationship between two sentences.\n",
    "\n",
    "- Classification task\n",
    "- Requires commonsense and world knowledge\n",
    "- Requires general natural language understanding\n",
    "- Requires fine-grained reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recognizing Textual Entailment (RTE)\n",
    "\n",
    "[Dagan et al., 2005](http://u.cs.biu.ac.il/~dagan/publications/RTEChallenge.pdf)\n",
    "\n",
    "- Text T\n",
    "- Hypothesis H\n",
    "\n",
    "T entails H if, typically, a human reading T would infer that H is most likely true.\n",
    "\n",
    "\n",
    "Positive:\n",
    "\n",
    "> “Google files for its long awaited IPO.”\n",
    "\n",
    "<center>\n",
    "$\\Rightarrow$\n",
    "</center>\n",
    "\n",
    "> “Google goes public.”\n",
    "\n",
    "Negative:\n",
    "\n",
    "> “Bush returned to the White House late Saturday while his running mate was off campaigning in the West.”\n",
    "\n",
    "<center>\n",
    "$\\not\\Rightarrow$\n",
    "</center>\n",
    "\n",
    "> “Bush left the White House.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stanford Natural Language Inference (SNLI) corpus\n",
    "\n",
    "[Bowman et al., 2015](https://nlp.stanford.edu/pubs/snli_paper.pdf)\n",
    "\n",
    "570K sentence pairs, two orders of magnitude larger than other NLI resources (1K-10K examples).\n",
    "\n",
    " **A wedding party taking pictures**\n",
    "- There is a funeral\t\t\t\t\t: **<span class=red>Contradiction</span>**\n",
    "- They are outside\t\t\t\t\t    : **<span class=blue>Neutral</span>**\n",
    "- Someone got married\t\t\t\t    : **<span class=green>Entailment</span>**\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/31/Wedding_photographer_at_work.jpg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-NLI (MNLI)\n",
    "\n",
    "[Williams et al., 2018](https://www.nyu.edu/projects/bowman/multinli/paper.pdf): more diverse domains.\n",
    "\n",
    "Entailment:\n",
    "\n",
    "> “The legislation was widely hailed as a model for the country.”\n",
    "\n",
    "<center>\n",
    "$\\Rightarrow$\n",
    "</center>\n",
    "\n",
    "> “Many people thought the legislation was a model for the country.”\n",
    "\n",
    "Neutral:\n",
    "\n",
    "> “The program has helped victims in 90 court cases, and 150 legal counseling sessions have been held there.”\n",
    "\n",
    "<center>\n",
    "?\n",
    "</center>\n",
    "\n",
    "> “Victims from 90 grand jury court cases were helped by the program.”\n",
    "\n",
    "Contradiction:\n",
    "\n",
    "> “As a result, Chris Schneider, executive director of Central California Legal Services, is building a lawsuit against Alpaugh Irrigation.”\n",
    "\n",
    "<center>\n",
    "$\\Rightarrow\\neg$\n",
    "</center>\n",
    "\n",
    "> “Central California Legal Services’ executive director decided not to pursue a lawsuit against Alpaugh Irrigation.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLUE benchmark\n",
    "\n",
    "[Wang et al., 2019](https://openreview.net/pdf?id=rJ4km2R5t7): collection of sentence- and sentence-pair-classification tasks.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/d9f6ada77448664b71128bb19df15765336974a6/2-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GLUE: Winograd NLI (WNLI)\n",
    "\n",
    "World knowledge and logical reasoning, presented as NLI.\n",
    "\n",
    "Positive:\n",
    "\n",
    "> “I put the cake away in the refrigerator. It has a lot of butter in it.”\n",
    "\n",
    "<center>\n",
    "$\\Rightarrow$\n",
    "</center>\n",
    "\n",
    "> “The cake has a lot of butter in it.”\n",
    "\n",
    "Negative:\n",
    "\n",
    "> “The large ball crashed right through the table because it was made of styrofoam.”\n",
    "\n",
    "<center>\n",
    "$\\not\\Rightarrow$\n",
    "</center>\n",
    "\n",
    "> “The large ball was made of styrofoam.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SuperGLUE\n",
    "\n",
    "[Wang et al., 2019](https://arxiv.org/pdf/1905.00537v2.pdf): harder NLI, for a meaningful comparison.\n",
    "\n",
    "> “Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.”\n",
    "\n",
    "<center>\n",
    "$\\not\\Rightarrow$\n",
    "</center>\n",
    "\n",
    "> “Christopher Reeve had an accident.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RTE/NLI state of the art until 2015\n",
    "\n",
    "[Lai and Hockenmaier, 2014](https://www.aclweb.org/anthology/S14-2055.pdf),\n",
    "[Jimenez et al., 2014](https://www.aclweb.org/anthology/S14-2131.pdf),\n",
    "[Zhao et al., 2014](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6889713),\n",
    "[Beltagy et al., 2016](https://www.aclweb.org/anthology/J16-4007.pdf) and others:\n",
    "engineered pipelines.\n",
    "\n",
    "- Various external resources\n",
    "- Specialized subcomponents\n",
    "- Extensive use of **features**:\n",
    "  - Negation detection, word overlap, part-of-speech tags, dependency parses, alignment, symbolic meaning representation\n",
    "  \n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/fca1e631b8f93036065311eb92727c509423475a/9-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural networks for NLI\n",
    "\n",
    "Large-scale NLI corpora: NNs are feasible to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independent sentence encoding\n",
    "\n",
    "[Bowman et al, 2015](https://www.aclweb.org/anthology/D15-1075.pdf): same LSTM encodes premise and hypothesis.\n",
    "\n",
    "<img src=\"dl-applications-figures/rte.svg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Last output vector as sentence representation.\n",
    "\n",
    "<img src=\"dl-applications-figures/rte_encoding.svg\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MLP to classify as entailment/neutral/contradiction.\n",
    "\n",
    "<img src=\"dl-applications-figures/mlp.svg\" width=700/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "| Model | k | θ<sub>W+M</sub> | θ<sub>M</sub> | Train | Dev | Test |\n",
    "|-|-|-|-|-|-|-|\n",
    "| LSTM | 100 | \\\\(\\approx\\\\)10M | 221k | 84.4 | - | 77.6|\n",
    "| Classifier| - | - | - | 99.7 | - | 78.2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional encoding\n",
    "\n",
    "The way we read the hypothesis could be influenced by our understanding of the premise.\n",
    "\n",
    "<img src=\"dl-applications-figures/conditional.svg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"dl-applications-figures/conditional_encoding.svg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "| Model | k | θ<sub>W+M</sub> | θ<sub>M</sub> | Train | Dev | Test |\n",
    "|-|-|-|-|-|-|-|\n",
    "| LSTM | 100 | \\\\(\\approx\\\\)10M | 221k | 84.4 | - | 77.6|\n",
    "| Classifier| - | - | - | 99.7 | - | 78.2|\n",
    "| Conditional endcoding | 159 | 3.9M | 252k | 84.4 | 83.0 | 81.4|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> You can’t cram the meaning of a whole\n",
    "%&!\\$# sentence into a single \\$&!#* vector!\n",
    ">\n",
    "> -- <cite>Raymond J. Mooney</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attention\n",
    "\n",
    "<img src=\"dl-applications-figures/attention.svg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"dl-applications-figures/attention_encoding.svg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img  src=\"./dl-applications-figures/camel.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Contextual understanding\n",
    "\n",
    "<img src=\"./dl-applications-figures/pink.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "| Model | k | θ<sub>W+M</sub> | θ<sub>M</sub> | Train | Dev | Test |\n",
    "|-|-|-|-|-|-|-|\n",
    "| LSTM | 100 | \\\\(\\approx\\\\)10M | 221k | 84.4 | - | 77.6|\n",
    "| Classifier| - | - | - | 99.7 | - | 78.2|\n",
    "| Conditional encoding | 159 | 3.9M | 252k | 84.4 | 83.0 | 81.4|\n",
    "| Attention | 100 | 3.9M | 242k | 85.4 | 83.2 | 82.3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fuzzy attention\n",
    "\n",
    "<img  src=\"./dl-applications-figures/mimes.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word-by-word attention\n",
    "\n",
    "<img src=\"dl-applications-figures/word_attention.svg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"dl-applications-figures/word_attention_encoding.svg\" width=800/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reordering\n",
    "\n",
    "<img src=\"./dl-applications-figures/reordering.png\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Synonyms\n",
    "\n",
    "<img  src=\"./dl-applications-figures/trashcan.png\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hypernyms\n",
    "\n",
    "<img src=\"./dl-applications-figures/kids.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lexical inference\n",
    "\n",
    "<img src=\"./dl-applications-figures/snow.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "| Model | k | θ<sub>W+M</sub> | θ<sub>M</sub> | Train | Dev | Test |\n",
    "|-|-|-|-|-|-|-|\n",
    "| LSTM | 100 | \\\\(\\approx\\\\)10M | 221k | 84.4 | - | 77.6|\n",
    "| Classifier| - | - | - | 99.7 | - | 78.2|\n",
    "| Conditional encoding | 159 | 3.9M | 252k | 84.4 | 83.0 | 81.4|\n",
    "| Attention | 100 | 3.9M | 242k | 85.4 | 83.2 | 82.3 |\n",
    "| Word-by-word attention | 100 | 3.9M | 252k | 85.3 | **83.7** | **83.5** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Composition\n",
    "\n",
    "[Bowman et al., 2016](https://www.aclweb.org/anthology/P16-1139.pdf):\n",
    "compositional vector representation based on syntactic structure.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/36c097a225a95735271960e2b63a2cb9e98bff83/1-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLI artefacts\n",
    "\n",
    "SNLI and MNLI are **crowdsourced**.\n",
    "\n",
    "[Gururangan et al., 2018](https://www.aclweb.org/anthology/N18-2017.pdf): hypothesis phrasing alone gives out the class.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/2997b26ffb8c291ce478bd8a6e47979d5a55c466/2-Table1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lexical entailment\n",
    "\n",
    "[Glockner et al., 2018](https://www.aclweb.org/anthology/P18-2103.pdf): very **simple** examples that are hard for models.\n",
    "\n",
    "<img src=\"https://persagen.com/files/misc/arxiv1805.02266-table1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer learning\n",
    "\n",
    "Reading material:\n",
    "\n",
    "- [The State of Transfer Learning in NLP](http://ruder.io/state-of-transfer-learning-in-nlp/)\n",
    "- [An Overview of Multi-Task Learning in Deep Neural Networks](https://arxiv.org/pdf/1706.05098.pdf)\n",
    "\n",
    "[Quiz results](https://absalon.instructure.com/courses/35895/quizzes/37714/statistics)\n",
    "\n",
    "<img src=\"https://ruder.io/content/images/2019/08/transfer_learning_taxonomy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A very brief history of transfer learning in NLP\n",
    "\n",
    "- [Collobert et al., 2011](https://arxiv.org/pdf/1103.0398.pdf): many tasks with shared embeddings.\n",
    "- [word2vec](https://arxiv.org/pdf/1301.3781.pdf), [GloVe](https://www.aclweb.org/anthology/D14-1162.pdf) and others:\n",
    "pre-trained word embeddings.\n",
    "- [ELMo](https://www.aclweb.org/anthology/N18-1202.pdf), [BERT](https://www.aclweb.org/anthology/N19-1423.pdf) and others:\n",
    "pre-trained contextualized embeddings.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/2538e3eb24d26f31482c479d95d2e26c0e79b990/3-Figure1-1.png\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-task learning in NLP\n",
    "\n",
    "- [Caruana, 1997](https://www.cs.cornell.edu/~caruana/mlj97.pdf): introduction of MTL.\n",
    "- [Hashimoto et al, 2017](https://www.aclweb.org/anthology/D17-1206.pdf),\n",
    "[Bjerva, 2017](https://www.aclweb.org/anthology/W17-0225.pdf),\n",
    "[Bollmann et al., 2018](https://www.aclweb.org/anthology/W18-3403.pdf),\n",
    "[Hershcovich et al., 2018](https://www.aclweb.org/anthology/P18-1035.pdf),\n",
    "[Augenstein et al., 2018](https://www.aclweb.org/anthology/N18-1172.pdf) and many others: MTL for NLP.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/ade0c116120b54b57a91da51235108b75c28375a/1-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLI and parsing\n",
    "\n",
    "[Bowman et al., 2016](https://www.aclweb.org/anthology/P16-1139.pdf):\n",
    "transition-based parsing and NLI. Stack-augmented Parser-Interpreter Neural Network (SPINN).\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/36c097a225a95735271960e2b63a2cb9e98bff83/2-Figure2-1.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-task\n",
    "\n",
    "[Augenstein et al., 2018](https://www.aclweb.org/anthology/N18-1172.pdf):\n",
    "sentiment analysis, stance detection, fake news detection and NLI\n",
    "with a Label Embedding Layer.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/64c5f7055b2e6982b6b95e069b22230d13a134bb/4-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-trained embeddings\n",
    "\n",
    "General-purpose representations trained on large datasets (usually unsupervised): \n",
    "\n",
    "- [word2vec](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [GloVe](https://www.aclweb.org/anthology/D14-1162.pdf)\n",
    "- [ELMo](https://www.aclweb.org/anthology/N18-1202.pdf)\n",
    "- [BERT](https://www.aclweb.org/anthology/N19-1423.pdf)\n",
    "\n",
    "are all forms of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLUE\n",
    "\n",
    "As a collection of structurally similar classification task, serves as a general language understanding benchmark.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/d9f6ada77448664b71128bb19df15765336974a6/2-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Character-Aware Neural Language Models\n",
    "\n",
    "[Kim et al., 2015](https://arxiv.org/pdf/1508.06615.pdf)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dsindex/blog/master/images/ngram_cnn_highway_1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ELMo\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/7/74/Elmo_from_Sesame_Street.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Alammar, 2018](http://jalammar.github.io/illustrated-bert/)\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/elmo-embedding-robin-williams.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Peters et al., 2018](https://www.aclweb.org/anthology/N18-1202.pdf):\n",
    "stacked LSTMs with BiLM (bidirectional language model) objective.\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://jalammar.github.io/images/elmo-embedding.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers\n",
    "\n",
    "[Vaswani et al., 2017](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf):\n",
    "self-attention: each word attends to all words.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/204e3073870fae3d05bcbc2f6a8e263d9b72e776/4-Figure2-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Self-attention network\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/204e3073870fae3d05bcbc2f6a8e263d9b72e776/3-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Long-distance dependencies\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/204e3073870fae3d05bcbc2f6a8e263d9b72e776/13-Figure3-1.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT\n",
    "\n",
    "[Devlin et al., 2019](https://www.aclweb.org/anthology/N19-1423.pdf):\n",
    "bidirectional self-attention with masked language model + next sentence prediction objectives.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/300/0*2XpE-VjhhLGkFDYg.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Masked language model\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/BERT-language-modeling-masked-lm.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Next sentence prediction\n",
    "\n",
    "**Conditional encoding** of both sentences.\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using BERT\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/bert-tasks.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which layer to use?\n",
    "\n",
    "<img src=\"http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Layer specialization\n",
    "\n",
    "[Tenney et al., 2019](https://www.aclweb.org/anthology/P19-1452.pdf): different layers are better at different tasks.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/groundai-web-prod/media/users/user_234892/project_363715/images/supplemental/bylayer_base.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "16GB of text from Wikipedia + BookCorpus.\n",
    "\n",
    "- Batch Size: 131,072 words (1024 sequences * 128\n",
    "length or 256 sequences * 512 length)\n",
    "- Training Time: 1M steps (~40 epochs)\n",
    "- BERT-Base: 12-layer, 768-hidden, 12-head\n",
    "- BERT-Large: 24-layer, 1024-hidden, 16-head\n",
    "- Trained on 4x4 or 8x8 TPU slice for 4 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RoBERTa\n",
    "\n",
    "[Liu et al., 2019](https://arxiv.org/pdf/1907.11692.pdf): bigger is better.\n",
    "\n",
    "BERT with additionally\n",
    "\n",
    "- CC-News (76GB)\n",
    "- OpenWebText (38GB)\n",
    "- Stories (31GB)\n",
    "\n",
    "and **no** next-sentence-prediction task (only masked LM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLUE\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/d9f6ada77448664b71128bb19df15765336974a6/2-Figure1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## T5\n",
    "\n",
    "[Raffel et al., 2019](https://arxiv.org/pdf/1910.10683.pdf): train on ALL the tasks!\n",
    "\n",
    "<img src=\"dl-applications-figures/t5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## T5\n",
    "\n",
    "Currently [best on GLUE](https://gluebenchmark.com/leaderboard)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further reading\n",
    "\n",
    "- [Linguistic Knowledge and Transferability of Contextual Representations](https://www.aclweb.org/anthology/N19-1112.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
