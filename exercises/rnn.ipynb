{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Exercise: Recognizing Textual Entailment\n",
    "\n",
    "In this exercise, we are going to implement some of the models discussed in the Deep Learning for Natural Language Processing chapter. Specifically, we are going to implement an RTE system using TensorFlow. Instead of running this on a large corpus like SNLI, we are working on a very small corpus for implementation purposes. This is a common practice as in early stages of development we will likely encounter bugs in our cope and compile and run-time errors for which we do not need to train on a lot of data. It is generally a good idea to first test whether our model can overfit a tiny debug corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-11T17:34:58.776194",
     "start_time": "2016-12-11T17:34:58.745553"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full([6, 15], 0) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/usr/local/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full([6, 12], 0) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# examples from SNLI training corpus, 2=entailment, 1=neutral, 0=contradiction\n",
    "# premise, hypothesis, label\n",
    "data = [\n",
    "    (\"Children smiling and waving at camera\", \"They are smiling at their parents\", 1),\n",
    "    (\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy does a skateboarding trick.\", 2),\n",
    "    (\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk.\", 0),\n",
    "    (\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\", 2),    \n",
    "    (\"A woman in a green jacket and hood over her head looking towards a valley.\", \"The woman is cold.\", 1),\n",
    "    (\"A couple playing with a little boy on the beach.\", \"A couple watch a little girl play by herself on the beach.\", 0)\n",
    "]\n",
    "\n",
    "def data2np(data, PAD=0):\n",
    "    \"\"\"Transforms data into a list of numpy tensors.\"\"\"\n",
    "    premises = []; premise_lengths = []\n",
    "    hypotheses = []; hypothesis_lengths = []\n",
    "    labels = []    \n",
    "    for premise, hypothesis, label in data:\n",
    "        premise_tokenized = premise.split(\" \")\n",
    "        premises.append(premise_tokenized)\n",
    "        premise_lengths.append(len(premise_tokenized))\n",
    "        hypotheses_tokenized = hypothesis.split(\" \")\n",
    "        hypotheses.append(hypotheses_tokenized)\n",
    "        hypothesis_lengths.append(len(hypotheses_tokenized))\n",
    "        labels.append(label)\n",
    "    vocab = {\"<PAD>\": PAD}\n",
    "    premises_np = np.full([len(data), np.max(premise_lengths)], PAD)\n",
    "    hypotheses_np = np.full([len(data), np.max(hypothesis_lengths)], PAD)    \n",
    "    for k, seqs in enumerate([premises, hypotheses]):\n",
    "        for i, seq in enumerate(seqs):\n",
    "            for j, word in enumerate(seq):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "                seq[j] = vocab[word] \n",
    "            if k == 0:\n",
    "                premises_np[i, 0:premise_lengths[i]] = seq\n",
    "            else:\n",
    "                hypotheses_np[i, 0:hypothesis_lengths[i]] = seq\n",
    "    return premises_np, premise_lengths, hypotheses_np, hypothesis_lengths, labels, vocab\n",
    "\n",
    "premises_np, premise_lengths, hypotheses_np, hypothesis_lengths, labels, vocab = data2np(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-11T17:50:33.461380",
     "start_time": "2016-12-11T17:50:31.444260"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.5509043, array([[ 0.33909366,  0.3227655 ,  0.33814088],\n",
      "       [ 0.32858452,  0.30642575,  0.3649897 ],\n",
      "       [ 0.32858452,  0.30642575,  0.3649897 ],\n",
      "       [ 0.33119491,  0.33293322,  0.33587188],\n",
      "       [ 0.3310172 ,  0.33154264,  0.33744016],\n",
      "       [ 0.33147478,  0.34967294,  0.31885228]], dtype=float32)]\n",
      "[6.1229205, array([[ 0.39692631,  0.34088796,  0.26218575],\n",
      "       [ 0.40245825,  0.2442368 ,  0.35330495],\n",
      "       [ 0.40245825,  0.2442368 ,  0.35330495],\n",
      "       [ 0.39924094,  0.3055864 ,  0.29517263],\n",
      "       [ 0.39399284,  0.36256841,  0.24343877],\n",
      "       [ 0.42257226,  0.33501515,  0.24241255]], dtype=float32)]\n",
      "[5.1039205, array([[ 0.32407063,  0.38640547,  0.28952384],\n",
      "       [ 0.32473934,  0.151014  ,  0.52424663],\n",
      "       [ 0.32473934,  0.151014  ,  0.52424663],\n",
      "       [ 0.33284369,  0.2670317 ,  0.40012458],\n",
      "       [ 0.29524836,  0.53090107,  0.17385054],\n",
      "       [ 0.43458319,  0.32423106,  0.24118577]], dtype=float32)]\n",
      "[3.9506092, array([[ 0.27758807,  0.43216571,  0.29024613],\n",
      "       [ 0.2781184 ,  0.06161524,  0.66026634],\n",
      "       [ 0.2781184 ,  0.06161524,  0.66026634],\n",
      "       [ 0.29961801,  0.16795205,  0.53242993],\n",
      "       [ 0.1982481 ,  0.73156786,  0.07018398],\n",
      "       [ 0.62252396,  0.25469556,  0.12278046]], dtype=float32)]\n",
      "[3.0498502, array([[ 0.19865304,  0.53728461,  0.26406235],\n",
      "       [ 0.30891114,  0.03340258,  0.65768623],\n",
      "       [ 0.30891114,  0.03340258,  0.65768623],\n",
      "       [ 0.26070943,  0.09999786,  0.63929266],\n",
      "       [ 0.11573428,  0.8662203 ,  0.01804551],\n",
      "       [ 0.78357774,  0.1504502 ,  0.06597207]], dtype=float32)]\n",
      "[2.3774743, array([[ 0.10364994,  0.70162725,  0.19472274],\n",
      "       [ 0.5459618 ,  0.02655807,  0.42748016],\n",
      "       [ 0.5459618 ,  0.02655807,  0.42748016],\n",
      "       [ 0.22536938,  0.07530551,  0.69932514],\n",
      "       [ 0.10435996,  0.89078975,  0.00485025],\n",
      "       [ 0.90957183,  0.06388866,  0.02653949]], dtype=float32)]\n",
      "[1.976503, array([[ 0.0383105 ,  0.84059578,  0.1210937 ],\n",
      "       [ 0.3999587 ,  0.01484168,  0.58519959],\n",
      "       [ 0.3999587 ,  0.01484168,  0.58519959],\n",
      "       [ 0.135923  ,  0.09728123,  0.76679581],\n",
      "       [ 0.0442766 ,  0.95367074,  0.00205263],\n",
      "       [ 0.96301067,  0.02503587,  0.01195341]], dtype=float32)]\n",
      "[1.7421181, array([[  1.38535956e-02,   9.13752496e-01,   7.23939240e-02],\n",
      "       [  4.36330885e-01,   9.87037737e-03,   5.53798676e-01],\n",
      "       [  4.36330885e-01,   9.87037737e-03,   5.53798676e-01],\n",
      "       [  9.51850712e-02,   7.56162703e-02,   8.29198658e-01],\n",
      "       [  2.84427069e-02,   9.70602274e-01,   9.55082942e-04],\n",
      "       [  9.85624194e-01,   9.58029460e-03,   4.79543069e-03]], dtype=float32)]\n",
      "[1.6939502, array([[  4.81477939e-03,   9.51426327e-01,   4.37589511e-02],\n",
      "       [  6.58526838e-01,   4.90101799e-03,   3.36572170e-01],\n",
      "       [  6.58526838e-01,   4.90101799e-03,   3.36572170e-01],\n",
      "       [  6.72829002e-02,   3.87829281e-02,   8.93934131e-01],\n",
      "       [  1.89226568e-02,   9.80633855e-01,   4.43410710e-04],\n",
      "       [  9.94231462e-01,   3.87522578e-03,   1.89342129e-03]], dtype=float32)]\n",
      "[1.6107647, array([[  1.58771768e-03,   9.72755969e-01,   2.56563127e-02],\n",
      "       [  3.35949123e-01,   5.60297491e-03,   6.58447921e-01],\n",
      "       [  3.35949123e-01,   5.60297491e-03,   6.58447921e-01],\n",
      "       [  3.83936316e-02,   2.13979017e-02,   9.40208495e-01],\n",
      "       [  9.12230927e-03,   9.90672648e-01,   2.05023840e-04],\n",
      "       [  9.96553302e-01,   2.15600338e-03,   1.29070051e-03]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    ## Placeholders\n",
    "    # [batch_size x max_premise_length]\n",
    "    premises_pl = tf.placeholder(tf.int64, [None, None], \"premises\")\n",
    "    # [batch_size]\n",
    "    premise_lengths_pl = tf.placeholder(tf.int64, [None], \"premise_lengths\")\n",
    "    # [batch_size x max_hypothesis_length]\n",
    "    hypotheses_pl = tf.placeholder(tf.int64, [None, None], \"hypotheses\")\n",
    "    # [batch_size]\n",
    "    hypothesis_lengths_pl = tf.placeholder(tf.int64, [None], \"hypothesis_lengths\")\n",
    "    # [batch_size]\n",
    "    labels_pl = tf.placeholder(tf.int64, [None], \"labels\")\n",
    "\n",
    "    ## Model\n",
    "    input_size = 2\n",
    "    hidden_size = 5\n",
    "    target_size = 3\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    embeddings = tf.get_variable(\"W\", [vocab_size, input_size])\n",
    "\n",
    "    # [batch_size x max_premise_length x input_size]\n",
    "    premises_embedded = tf.nn.embedding_lookup(embeddings, premises_pl)\n",
    "    # [batch_size x max_hypothesis_length x input_size]\n",
    "    hypotheses_embedded = tf.nn.embedding_lookup(embeddings, premises_pl)\n",
    "\n",
    "    with tf.variable_scope(\"encoder\") as varscope:\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "        _, premise_final_state = \\\n",
    "            tf.nn.dynamic_rnn(cell, premises_embedded, sequence_length=premise_lengths_pl, dtype=tf.float32)    \n",
    "        premises_h = premise_final_state.h\n",
    "        varscope.reuse_variables()  # using the same encoder for premises and hypotheses\n",
    "        _, hypothesis_final_state = \\\n",
    "            tf.nn.dynamic_rnn(cell, hypotheses_embedded, sequence_length=hypothesis_lengths_pl, dtype=tf.float32)    \n",
    "        hypotheses_h = hypothesis_final_state.h\n",
    "         \n",
    "    # [batch_size x 2*hidden_size]\n",
    "    pair_representation = tf.concat(1, [premises_h, hypotheses_h])\n",
    "        \n",
    "    # [batch_size x target_size]\n",
    "    logits = tf.contrib.layers.linear(pair_representation, target_size)\n",
    "    \n",
    "    probability = tf.nn.softmax(logits)\n",
    "    \n",
    "    ## Training Loss\n",
    "    loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels))\n",
    "            \n",
    "    ## Optimizer\n",
    "    optim = tf.train.AdamOptimizer(0.1)\n",
    "    optim_op = optim.minimize(loss)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        feed_dict = {\n",
    "            premises_pl: premises_np,\n",
    "            premise_lengths_pl: premise_lengths,\n",
    "            hypotheses_pl: hypotheses_np,\n",
    "            hypothesis_lengths_pl: hypothesis_lengths,\n",
    "            labels_pl: labels\n",
    "        }\n",
    "        \n",
    "        for i in range(10):\n",
    "            result = sess.run([optim_op, loss, probability], feed_dict)\n",
    "            print(result[1:])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
