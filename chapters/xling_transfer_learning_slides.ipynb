{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n",
    "<style>\n",
    ".rendered_html td {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    ".rendered_html th {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-lingual Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "+ Transfer learning\n",
    "+ Annotation projection\n",
    "+ Multilingual models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linguistic diversity and inclusion\n",
    "\n",
    "Language distribution in the world is highly skewed:\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/0e141942fa265142f41a2a26eb17b6005d3af29e/4-Table1-1.png\" width=90%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/2020.acl-main.560/\">Joshi et al., 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NLP across the resource landscape\n",
    "\n",
    "Resource distribution is skewed as well:\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/0e141942fa265142f41a2a26eb17b6005d3af29e/3-Figure2-1.png\" width=65%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/2020.acl-main.560/\">Joshi et al., 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"dl-applications-figures/WS_mapping.png\" width=\"100%\"/>\n",
    "\n",
    "Source: http://ai.stanford.edu/blog/weak-supervision/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer learning\n",
    "\n",
    "Overall taxonomy of transfer learning approaches in NLP:\n",
    "<img src=\"https://ruder.io/content/images/2019/08/transfer_learning_taxonomy.png\" width=50%>\n",
    "\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64\">Ruder, 2019</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-task learning\n",
    "\n",
    "Training one model with multiple objectives:\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/161ffb54a3fdf0715b198bb57bd22f910242eb49/19-Figure1.2-1.png\" width=40%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://www.cs.cornell.edu/~caruana/mlj97.pdf\">Caruana, 1997</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Weight sharing can happen on different levels:\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/ade0c116120b54b57a91da51235108b75c28375a/1-Figure1-1.png\" width=50%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://www.aclweb.org/anthology/D17-1206\">Hashimoto et al, 2017</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Domain adaptation\n",
    "\n",
    "*Domain*: intuitively, a collection of texts from a certain coherent sort of discourse ([Plank, 2011](https://pure.rug.nl/ws/portalfiles/portal/10469718/09complete.pdf)).\n",
    "\n",
    "For example, product reviews for DVDs, electronics, and kitchen goods:\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/e505303ba5287f468773fbef22ab1abf6875efca/1-Figure1-1.png\" width=45%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/2020.emnlp-main.639/\">Wright & Augenstein, 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Domain adaptation methods\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/11342d45911ee8a7c9e3a94117ce774ad7036172/2-Figure1-1.png\" width=80%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/2020.coling-main.603/\">Ramponi & Plank, 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A more general view on generalization\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/cc006a0790ba6fcd994f051e4ebb1af1ad322cc2/9-Figure3-1.png\" width=80%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/abs/2210.03050\">Hupkes et al., 2022</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/cc006a0790ba6fcd994f051e4ebb1af1ad322cc2/4-Figure1-1.png\" width=80%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/abs/2210.03050\">Hupkes et al., 2022</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-training\n",
    "\n",
    "General-purpose representations trained on large datasets enable sequential transfer learning.\n",
    "\n",
    "Type-level representations ([slides](dl-representations_simple.ipynb):\n",
    "- [word2vec](https://arxiv.org/abs/1301.3781)\n",
    "- [GloVe](https://www.aclweb.org/anthology/D14-1162)\n",
    "- ...\n",
    "\n",
    "Token-level (contextualized) representations ([slides](dl-representations_contextual.ipynb)):\n",
    "- [ELMo](https://www.aclweb.org/anthology/N18-1202)\n",
    "- [BERT](https://www.aclweb.org/anthology/N19-1423)\n",
    "- [BART](https://aclanthology.org/2020.acl-main.703/)\n",
    "- [T5](https://arxiv.org/abs/1910.10683)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text-to-text multitask pre-training and fine-tuning\n",
    "\n",
    "GPT-2: language models are unsupervised multitask learners ([Radford et al., 2018](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)).\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://1.bp.blogspot.com/-89OY3FjN0N0/XlQl4PEYGsI/AAAAAAAAFW4/knj8HFuo48cUFlwCHuU5feQ7yxfsewcAwCLcBGAsYHQ/s640/image2.png\" width=50%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/abs/1910.10683\">Raffel et al., 2019</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text-to-text multitask pre-training and zero-shot prompting\n",
    "\n",
    "GPT-3: language models are few-shot learners ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)).\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://raw.githubusercontent.com/yongzx/bigscience-workshop.github.io/gh-pages/en/pages/uploads/images/Octopus.png\" width=50%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/abs/2110.08207\">Sanh et al., 2021</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-lingual transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Strategies\n",
    "\n",
    "* Multilingual models\n",
    "* Translate-train\n",
    "* Translate-test\n",
    "* In-language\n",
    "* Multi-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strategies for cross-lingual transfer\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/96c7f39c358343b6ea412697ed693fdd04a71516/2-Figure1-1.png\" width=\"90%\">\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/W18-0550/\">Horbach et al., 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Zero-shot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Zero-shot cross-lingual transfer\n",
    "\n",
    "1. Pre-train (or download) mBERT etc.\n",
    "2. Fine-tune on a task in one language (e.g., English)\n",
    "3. Test on the same task in another language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zero-shot cross-lingual transfer\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/f6e181c0bd967e9b797d09c579f2ad3fccdbacd2/2-Figure1-1.png\" width=100%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/2020.acl-main.421/\">Artetxe et al., 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multilingual pre-training\n",
    "\n",
    "- [MUSE](https://openreview.net/forum?id=H196sainb)\n",
    "- [mBERT](https://www.aclweb.org/anthology/N19-1423)\n",
    "- [XLM-R](https://aclanthology.org/2020.acl-main.747/)\n",
    "- [mBART](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00343/96484/Multilingual-Denoising-Pre-training-for-Neural)\n",
    "- [mT5](https://arxiv.org/abs/2010.11934)\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://ruder.io/content/images/size/w2000/2016/10/zou_et_al_2013.png\" width=50%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=https://aclanthology.org/D13-1141/\">Zou et al., 2013</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "mBERT is unreasonably effective at cross-lingual transfer!\n",
    "\n",
    "NER F1:\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/809cc93921e4698bde891475254ad6dfba33d03b/2-Table1-1.png\" width=80%/>\n",
    "</center>\n",
    "\n",
    "POS accuracy:\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/809cc93921e4698bde891475254ad6dfba33d03b/2-Table2-1.png\" width=80%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://www.aclweb.org/anthology/P19-1493.pdf\">Pires et al., 2019</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why? (poll)\n",
    "\n",
    "See also [K et al., 2020](https://arxiv.org/pdf/1912.07840.pdf);\n",
    "[Wu and Dredze., 2019](https://www.aclweb.org/anthology/D19-1077.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Massively multilingual encoders\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/74276a37bfa50f90dfae37f767b2b67784bd402a/3-Table1-1.png\" width=80%>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/2021.naacl-main.41/>Xue et al., 2021</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise: multilingual pre-training for MT\n",
    "Consider a machine translation system using a multilingual pre-trained encoder and decoder.\n",
    "<a href=\"https://docs.google.com/forms/d/14osHSWt2D3cpg4QTbqm3wHkYtSzLWcVeRMhymQdMZj0/edit\"><img src=\"../img/vauquois-2021.png\"></a>\n",
    "\n",
    "+ It depends on the encoding and model.\n",
    "+ If the performance is good, the attention should incorporate also pragmatic differences. But it might be difficult in practice.\n",
    "+ Because it encodes the input into a numerical representation of the meaning that is language-independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multilingual evaluation benchmarks\n",
    "\n",
    "Collections of multilingual multi-task benchmarks:\n",
    "\n",
    "* XGLUE (<a href=\"https://aclanthology.org/2020.emnlp-main.484/\">Liang et al., 2020</a>)\n",
    "* XTREME (<a href=\"https://arxiv.org/abs/2003.11080\">Hu et al., 2020</a>): includes **TyDiQA**\n",
    "* XTREME-R (<a href=\"https://aclanthology.org/2021.emnlp-main.802/\">Ruder et al., 2021</a>) \n",
    "\n",
    "<center>\n",
    "    <a href=\"https://sites.research.google/xtreme\">\n",
    "    <img src=\"https://1.bp.blogspot.com/-5J6e2txWChk/XpSc_BaYFnI/AAAAAAAAFss/QCLROHrEutAN3GvOyfRzK8J7DA9yLY5GACLcBGAsYHQ/s640/XTREME%2BStill%2Bart_04%2Broboto.png\" width=\"90%\">\n",
    "    </a>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/abs/2003.11080\">Hu et al., 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### XTREME\n",
    "\n",
    "<center>\n",
    "    <img src=\"dl-applications-figures/xtreme.png\" width=\"70%\">\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/abs/2003.11080\">Hu et al., 2020</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A note about culture\n",
    "\n",
    "<center>\n",
    "    <img src=\"../img/x-cultural_axes.png\" width=70%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://aclanthology.org/2022.acl-long.482/\">Hershcovich et al., 2022</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further reading\n",
    "\n",
    "- [Ruder, 2017. An Overview of Multi-Task Learning in Deep Neural Networks](https://ruder.io/multi-task/)\n",
    "- [Ruder, 2019. The State of Transfer Learning in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)\n",
    "- [Ruder, 2016. A survey of cross-lingual word embedding models](https://ruder.io/cross-lingual-embeddings/)\n",
    "- [Søgaard, Anders; Vulic, Ivan; Ruder, Sebastian; Faruqui, Manaal. 2019. Cross-lingual word embeddings](https://www.morganclaypool.com/doi/abs/10.2200/S00920ED2V01Y201904HLT042)\n",
    "- [Wang, 2019. Cross-lingual transfer learning.](https://shanzhenren.github.io/csci-699-replnlp-2019fall/lectures/W6-L3-Cross_Lingual_Transfer.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
