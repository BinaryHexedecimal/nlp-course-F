{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n",
       "<style>\n",
       ".rendered_html td {\n",
       "    font-size: xx-large;\n",
       "    text-align: left; !important\n",
       "}\n",
       ".rendered_html th {\n",
       "    font-size: xx-large;\n",
       "    text-align: left; !important\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n",
    "<style>\n",
    ".rendered_html td {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    ".rendered_html th {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "+ Natural language inference\n",
    "\n",
    "+ Cross-attention\n",
    "\n",
    "+ Self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![bears](../img/bears.png)\n",
    "\n",
    "**Given:**\n",
    "There are six bears. Three brown bears, a black bear and a pink bear run along the grass.\n",
    "\n",
    "Which of the following is correct?\n",
    "1. Some bears run\n",
    "2. All bears sit\n",
    "3. One bear sits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Task: Natural Language Inference\n",
    "\n",
    "Determining the logical relationship between two sentences, a **premise** and a **hypothesis**.\n",
    "\n",
    "Also known as *Recognising Textual Entailment* ([Dagan et al., 2005](http://u.cs.biu.ac.il/~nlp/downloads/publications/RTEChallenge.pdf)).\n",
    "\n",
    "We define entailment as:\n",
    "P entails H if a human reading P would typically infer that H is most likely true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- (Pairwise) sequence classification task\n",
    "- Requires commonsense and world knowledge\n",
    "- Requires general natural language understanding\n",
    "- Requires fine-grained reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **P:** “Google files for its long awaited IPO.”\n",
    "> **H:** “Google goes public.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Positive ($\\Rightarrow$, entails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stanford Natural Language Inference (SNLI) dataset\n",
    "\n",
    "Crowdsourced annotations for 570K sentence pairs using image captions ([Bowman et al., 2015](https://www.aclweb.org/anthology/D15-1075.pdf)).\n",
    "\n",
    "**P**: A wedding party taking pictures\n",
    "- **H:** There is a funeral\t\t\t\t\t: **<span class=red>Contradiction</span>** ($\\Rightarrow\\neg$)\n",
    "- **H:** They are outside\t\t\t\t\t    : **<span class=blue>Neutral</span>** (?)\n",
    "- **H:** Someone got married\t\t\t\t    : **<span class=green>Entailment</span>** ($\\Rightarrow$)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/31/Wedding_photographer_at_work.jpg\" width=1500/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representing sentences as vectors\n",
    "\n",
    "1. Encode premise and hypothesis\n",
    "2. Concatenate the representations\n",
    "3. Classify with MLP\n",
    "\n",
    "<center>\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/f04df4e20a18358ea2f689b4c129781628ef7fc1/7-Figure3-1.png\"/>\n",
    "</center>\n",
    "\n",
    "([Bowman et al., 2015](https://www.aclweb.org/anthology/D15-1075))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to represent a sentence with a vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The same LSTM encodes the premise and hypothesis.\n",
    "\n",
    "<img src=\"dl-applications-figures/rte.svg\" width=1500/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use the last hidden vectors of the LSTM as sentence representations.\n",
    "\n",
    "<img src=\"dl-applications-figures/rte_encoding.svg\" width=1500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SNLI results\n",
    "\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| LSTM | 77.6 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem 1: \n",
    "\n",
    "Asymmetry of premise and hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/f04df4e20a18358ea2f689b4c129781628ef7fc1/7-Figure3-1.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conditional encoding\n",
    "\n",
    "<img src=\"dl-applications-figures/conditional.svg\" width=1500/>\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"dl-applications-figures/conditional_encoding.svg\" width=1500/>\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SNLI results\n",
    "\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| LSTM | 77.6 |\n",
    "| LSTMs with conditional encoding | 80.9 |\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem 2: global memory\n",
    "\n",
    "Some words are more important to focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img  src=\"./dl-applications-figures/pink.png\"/>\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Attention\n",
    "\n",
    "<img src=\"dl-applications-figures/attention.svg\" width=1500/>\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention mechanism\n",
    "\n",
    "+ Original motivation: machine translation ([Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)); see [later lecture in the course](nmt_slides_active.ipynb)\n",
    "\n",
    "#### Idea\n",
    "\n",
    "+ A **weighted sum** of encoder hidden states is a differentiable function and has a fixed dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"dl-applications-figures/attention_encoding.svg\" width=1500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What is happening here?\n",
    "\n",
    "For the final prediction,\n",
    "+ Attention takes all premise hidden vectors $(\\mathbf{h}_1, \\ldots, \\mathbf{h}_n)$ as well as the final hypothesis hidden vector ($\\mathbf{h}_N$) as input\n",
    "+ Calculates probability distribution $\\alpha$ over premise hidden vectors using a softmax\n",
    "+ Combines $\\mathbf{h}_N$ with an $\\alpha$-weighted average of all premise hidden vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SNLI results\n",
    "\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| LSTM | 77.6 |\n",
    "| LSTMs with conditional encoding | 80.9 |\n",
    "| LSTMs with conditional encoding + attention | 82.3 |\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem 3: representation bottleneck\n",
    "\n",
    "> You can’t cram the meaning of a whole\n",
    "`%&!$#` sentence into a single `$&!#*` vector!\n",
    ">\n",
    "> -- <cite>Raymond J. Mooney</cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Alignment\n",
    "\n",
    "+ Non-neural models often use **alignment** between sequences\n",
    "\n",
    "<img  src=\"./dl-applications-figures/snow.png\"/>\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word-by-word Attention\n",
    "\n",
    "+ Computing attention for each hypothesis token can give us a **soft alignment**\n",
    "\n",
    "<img src=\"dl-applications-figures/word_attention.svg\" width=1500/>\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"dl-applications-figures/word_attention_encoding.svg\" width=1500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What is happening here?\n",
    "\n",
    "**For each hypothesis token $x_t$,**\n",
    "+ Attention takes all premise hidden vectors $(\\mathbf{h}_1, \\ldots, \\mathbf{h}_n)$ as well as the current hypothesis hidden vector ($\\mathbf{h}_t$) as input\n",
    "+ Generates probability distribution $\\alpha_t$ over all premise hidden vectors\n",
    "+ Uses a weighted average (by $\\alpha_t$) of all premise hidden vectors as input for the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SNLI results\n",
    "\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| LSTM | 77.6 |\n",
    "| LSTMs with conditional encoding | 80.9 |\n",
    "| LSTMs with conditional encoding + attention | 82.3 |\n",
    "| LSTMs with word-by-word attention | 83.5 |\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Attention matrix\n",
    "\n",
    "<img  src=\"./dl-applications-figures/snow.png\"/>\n",
    "\n",
    "([Rocktäschel et al., 2015](https://arxiv.org/abs/1509.06664))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally:\n",
    "\n",
    "<div class=small>\n",
    "\\begin{align}\n",
    "  \\mathbf{M}_t &= \\tanh(\\mathbf{W}^y\\mathbf{Y}+(\\mathbf{W}^h\\mathbf{h}_t+\\mathbf{W}^r\\mathbf{r}_{t-1})\\mathbf{1}^T_L) & \\mathbf{M}_t &\\in\\mathbb{R}^{k\\times L}\\\\\n",
    "  \\alpha_t &= \\text{softmax}(\\mathbf{w}^T\\mathbf{M}_t)&\\alpha_t&\\in\\mathbb{R}^L\\\\\n",
    "  \\mathbf{r}_t &= \\mathbf{Y}\\alpha^T_t + \\tanh(\\mathbf{W}^t\\mathbf{r}_{t-1})&\\mathbf{r}_t&\\in\\mathbb{R}^k\n",
    "\\end{align}\n",
    "</div>\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathbf{Y}$ is the concatenation of all premise hidden vectors\n",
    "* $\\mathbf{W}^y$, $\\mathbf{W}^h$, $\\mathbf{W}^r \\in\\mathbb{R}^{k\\times k}$ are trained projection matrices\n",
    "* $\\mathbf{w}$ is a trained parameter vector\n",
    "* $\\alpha_t$ is the attention probability distribution\n",
    "* $\\mathbf{r}_t$ is the weighted representation of the premise (dependent on $\\mathbf{r}_{t-1}$ to inform the model about what was attended over in the previous step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Final pairwise sentence representation:\n",
    "\n",
    "<div class=small>\n",
    "\\begin{align}\n",
    "  \\mathbf{h}^{*} &= \\text{tanh} (\\mathbf{W}^p\\mathbf{r}_N + \\mathbf{W}^x\\mathbf{h}_N)\n",
    "\\end{align}\n",
    "</div>\n",
    "\n",
    "Non-linear combination of the attention-weighted representation $\\mathbf{r}_t$ and the last output vector $\\mathbf{h}_N$, where $\\mathbf{h}^{*} \\in\\mathbb{R}^{k}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An important caveat\n",
    "\n",
    "+ The attention mechanism was motivated by the idea of aligning inputs & outputs\n",
    "+ Attention matrices often correspond to human intuitions about alignment\n",
    "+ But ***producing a sensible alignment is not a training objective!***\n",
    "\n",
    "In other words:\n",
    "\n",
    "+ Do not expect that attention weights will *necessarily* correspond to sensible alignments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem 4: attention only in one direction\n",
    "\n",
    "Hypothesis tokens attend to premise tokens.\n",
    "\n",
    "Why don't hypothesis tokens also attend to other **hypthesis** tokens?\n",
    "\n",
    "Why don't premise tokens also attend to **hypthesis** tokens?\n",
    "\n",
    "Why don't premise tokens attend to other **premise** tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention is all you need\n",
    "\n",
    "*Transformers* replace the whole LSTM with *self-attention* ([Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All tokens attend to each other:\n",
    "\n",
    "<center>\n",
    "    <img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\" width=40%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"http://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "Use hidden representation $\\mathbf{h}_i$ to create three vectors:\n",
    "query vector $\\color{purple}{\\mathbf{q}_i}=W^q\\mathbf{h}_i$,\n",
    "key vector $\\color{orange}{\\mathbf{k}_i}=W^k\\mathbf{h}_i$,\n",
    "value vector $\\color{blue}{\\mathbf{v}_i}=W^v\\mathbf{h}_i$.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\alpha}_{i,j} = \\text{softmax}\\left(\n",
    "\\frac{\\color{purple}{\\mathbf{q}_i}^\\intercal\n",
    "\\color{orange}{\\mathbf{k}_j}}\n",
    "{\\sqrt{d_{\\mathbf{h}}}}\n",
    "\\right) \\\\\n",
    "\\mathbf{h}_i^\\prime = \\sum_{j=1}^n \\mathbf{\\alpha}_{i,j} \\color{blue}{\\mathbf{v}_j}\n",
    "$$\n",
    "\n",
    "$W^q$, $W^k$ and $W^v$ are all trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In matrix form:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)=\n",
    "\\text{softmax}\\left(\n",
    "\\frac{\\color{purple}{Q}\n",
    "\\color{orange}{K}^\\intercal}\n",
    "{\\sqrt{d_{\\mathbf{h}}}}\n",
    "\\right) \\color{blue}{V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi-head self-attention\n",
    "\n",
    "Repeat this multiple times with multiple sets of parameter matrices, then concatenate:\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/multi_head_self_att.png\" width=30%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Vaswani et al., 2017</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h)W^O\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\text{head}_i=\\text{Attention}(QW_i^q,KW_i^k,VW_i^v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformer unit\n",
    "\n",
    "Add residual connections, layer normalization and feed-forward layers (MLPs):\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/transformer_layer.png\" width=30%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Vaswani et al., 2017</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformer\n",
    "\n",
    "Repeat this for multiple layers, each using the previous as input:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}^\\ell(Q^\\ell,K^\\ell,V^\\ell)=\\text{Concat}(\\text{head}_1^\\ell,\\ldots,\\text{head}_h^\\ell)W_\\ell^O\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\text{head}_i^\\ell=\\text{Attention}(Q^\\ell W_{i,\\ell}^q,K^\\ell W_{i,\\ell}^k,V^\\ell W_{i,\\ell}^v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Long-distance dependencies\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/ldd.png\" width=80%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Vaswani et al., 2017</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Back to bag-of-words?\n",
    "\n",
    "RNNs process tokens sequentially, but\n",
    "Transformers process all tokens **at once**.\n",
    "\n",
    "In fact, we did not even provide any information about the order of tokens..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Positional Encoding ##\n",
    "\n",
    "Represent **positions** with fixed-length vectors, with the same dimensionality as word embeddings:\n",
    "\n",
    "(1st position, 2nd position, 3rd position, ...) $\\to$ Must decide on maximum sequence length\n",
    "\n",
    "Add to word embeddings at the input layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/positional_1.png?0.7222289032042848\" width=\"1200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../img/positional_1.png'+'?'+str(random.random()), width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Positional Encoding ##\n",
    "\n",
    "Alternatives:\n",
    "\n",
    "* Learned position embeddings (like word embeddings)\n",
    "* **Static position encoding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/positional_2.png?0.19646694610556226\" width=\"1200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../img/positional_2.png'+'?'+str(random.random()), width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Picture source: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The full transformer model\n",
    "\n",
    "Deep multi-head self-attention encoder-decoder with sinusodial positional encodings:\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/transformer.png\" width=30%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Vaswani et al., 2017</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SNLI results\n",
    "\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| LSTM | 77.6 |\n",
    "| LSTMs with conditional encoding | 80.9 |\n",
    "| LSTMs with conditional encoding + attention | 82.3 |\n",
    "| LSTMs with word-by-word attention | 83.5 |\n",
    "| Self-attention | 85.6 |\n",
    "\n",
    "([Shen et al., 2018](https://dl.acm.org/doi/abs/10.5555/3504035.3504703))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformers for decoding\n",
    "\n",
    "Attends to encoded input *and* to partial output.\n",
    "\n",
    "<center>\n",
    "    <img src=\"http://jalammar.github.io/images/xlnet/transformer-encoder-decoder.png\" width=70%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"http://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can only attend to already-generated tokens.\n",
    "\n",
    "<center>\n",
    "    <img src=\"http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\" width=80%/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "    (from <a href=\"http://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The encoder transformer is sometimes called \"bidirectional transformer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "+ The **attention mechanism** alleviates the encoding bottleneck in encoder-decoder architectures\n",
    "\n",
    "+ Attention can even replace (bi)-LSTMs, giving **self-attention**\n",
    "\n",
    "+ **Transformers** rely on self-attention for encoding and decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "+ [Jurafsky & Martin Chapter 10, from 10.2 to 10.4 (including)](https://web.stanford.edu/~jurafsky/slp3/10.pdf); [9.7 in Chapter 9](https://web.stanford.edu/~jurafsky/slp3/9.pdf)\n",
    "+ Lilian Weng's blog post [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "+ Jay Alammar's blog post [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
