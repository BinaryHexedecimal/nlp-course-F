{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-66af89a9672e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"..\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mstatnlpbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mutil\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mstatnlpbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msequence\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mseq\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mstatnlpbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgmb\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mload_gmb_dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/stat-nlp-book/statnlpbook/sequence.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mcollections\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdefaultdict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mgraphviz\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mgv\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util\n",
    "import statnlpbook.sequence as seq\n",
    "from statnlpbook.gmb import load_gmb_dataset\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 5.0)\n",
    "from collections import defaultdict, Counter\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html td {\n",
       "    font-size: xx-large;\n",
       "    text-align: left; !important\n",
       "}\n",
       ".rendered_html th {\n",
       "    font-size: xx-large;\n",
       "    text-align: left; !important\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".rendered_html td {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    ".rendered_html th {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tikzmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext tikzmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Jabberwocky\n",
    "\n",
    "> â€™Twas brillig, and the slithy toves  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Did gyre and gimble in the wabe:  \n",
    "All mimsy were the borogoves,  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;And the mome raths outgrabe.\n",
    "\n",
    "Exercise:\n",
    "https://ucph.padlet.org/dh/jw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sequence Labeling\n",
    "\n",
    "+ Assigning exactly one label to each element in a sequence\n",
    "\n",
    "+ In context of RNNs, example of **one-to-one** paradigm\n",
    "\n",
    "<center><img src=\"../img/one_to_one.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "Assign each token in a sentence its **part-of-speech (POS) tag**.\n",
    "\n",
    "| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "|-|-|-|-|-|-|-|\n",
    "| I | predict | that | it | will | rain | tonight |\n",
    "| PRP | VBP | IN | PRP | MD | VB | NN |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part-of-speech tagsets\n",
    "\n",
    "+ POS tags group words with **similar grammatical properties**\n",
    "+ Granularity of tags can differ\n",
    "+ *For example:* English Penn TreeBank Tagset distinguishes four types of nouns\n",
    "\n",
    "| | | |\n",
    "|-|-|-|\n",
    "| **NN** | noun, singular or mass | *cat, rain* |\n",
    "| **NNS** | noun, plural | *cats, tables* |\n",
    "| **NNP** | proper noun, singular | *John, IBM* |\n",
    "| **NNPS** | proper noun, plural | *Muslims, Philippines* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Crucial challenge: Ambiguity!\n",
    "\n",
    "| | | | | | |\n",
    "|-|-|-|-|-|-|\n",
    "| He | is | treated | for | **back** | injury |\n",
    "| PRP | VBP | VBN | IN | **NN** | NN |\n",
    "\n",
    "| | | | | | |\n",
    "|-|-|-|-|-|-|\n",
    "| He | is | sent | **back** | to | prison |\n",
    "| PRP | VBP | VBN | **RB** | TO | NN |\n",
    "\n",
    "| | | | | |\n",
    "|-|-|-|-|-|\n",
    "| I | can | **back** | this | up | \n",
    "| PRP | MD | **VB** | DT | RP |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GMB dataset\n",
    "\n",
    "+ GMB = Groningen Meaning Bank\n",
    "+ https://www.kaggle.com/shoumikgoswami/annotated-gmb-corpus/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_gmb_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-c38952a228d9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtokens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ments\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_gmb_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/gmb/GMB_dataset_utf8.txt'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtokens\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'load_gmb_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokens, pos, ents = load_gmb_dataset('../data/gmb/GMB_dataset_utf8.txt')\n",
    "\n",
    "pd.DataFrame([tokens[2], pos[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "examples = {}\n",
    "counts = Counter(tag for sent in pos for tag in sent)\n",
    "words = defaultdict(set)\n",
    "for x_s, y_s in zip(tokens, pos):\n",
    "    for i, (x, y) in enumerate(zip(x_s, y_s)):\n",
    "        if y not in examples or random() > 0.97:\n",
    "            examples[y] = [x_s[j] + \"/\" + y_s[j] if i == j else x_s[j] for j in range(max(i-1,0),min(i+2,len(x_s)))]\n",
    "        words[y].add(x)\n",
    "sorted_tags = sorted(counts.items(),key=lambda x:-x[1])\n",
    "sorted_tags_with_examples = [(t,c,len(words[t]),\" \".join(examples[t])) for t,c in sorted_tags]\n",
    "\n",
    "sorted_tags_table = pd.DataFrame(sorted_tags_with_examples, columns=['Tag','Count','Unique Tokens','Example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tags_table[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The Penn Treebank tagset has 45 tags in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence Labeling as Structured Prediction\n",
    "\n",
    "* Input Space $\\Xs$: sequences of items to label\n",
    "* Output Space $\\Ys$: sequences of output labels\n",
    "* Model: $s_{\\params}(\\x,\\y)$\n",
    "* Prediction: $\\argmax_\\y s_{\\params}(\\x,\\y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Models\n",
    "Model probability distributions over label sequences $\\y$ conditioned on input sequences $\\x$\n",
    "\n",
    "$$\n",
    "s_{\\params}(\\x,\\y) = \\prob_\\params(\\y|\\x)\n",
    "$$ \n",
    "\n",
    "* Analogous to conditional models of [text classification](doc_classify_slides_short.ipynb) chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Local Models / Classifiers\n",
    "A **fully factorised** or **local** model:\n",
    "\n",
    "$$\n",
    "p_\\params(\\y|\\x) = \\prod_{i=1}^n p_\\params(y_i|\\x,i)\n",
    "$$\n",
    "\n",
    "* Labels are independent of each other\n",
    "* Inference in this model is trivial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "$$\n",
    "\\prob_\\params(\\text{\"PRP MD VB\"} \\bar \\text{\"it will rain\"}) = \\\\\\\\ \\prob_\\params(\\text{\"PRP\"}\\bar \\text{\"it will rain\"},1) \\cdot \\prob_\\params(\\text{\"MD\"} \\bar \\text{\"it will rain\"},2) \\cdot \\prob_\\params(\\text{\"VB\"} \\bar \\text{\"it will rain\"},3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametrisation\n",
    "\n",
    "**Log-linear classifier** $p_\\params(y\\bar\\x,i)$ to predict class for sentence $\\x$ and position $i$\n",
    "\n",
    "$$\n",
    "  p_\\params(y\\bar\\x,i) = \\frac{1}{Z_\\x} \\exp \\langle \\repr(\\x,i),\\params_y \\rangle\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ $\\repr(\\x,i)$ is a **feature function**\n",
    "+ How far can we get with very simple features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bias:\n",
    "$$\n",
    "\\repr_0(\\x,i) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Word at token to tag:\n",
    "$$\n",
    "\\repr_w(\\x,i) = \\begin{cases}1 \\text{ if }x_i=w \\\\\\\\ 0 \\text{ else} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def feat_1(x,i):\n",
    "    return {\n",
    "        'bias': 1.0,  \n",
    "        'word:' + x[i]: 1.0,\n",
    "    }\n",
    "\n",
    "train = list(zip(tokens[:-200], pos[:-200]))\n",
    "dev = list(zip(tokens[-200:], pos[-200:]))\n",
    "\n",
    "local_1 = seq.LocalSequenceLabeler(feat_1, train, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can assess the accuracy of this model on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "seq.accuracy(dev, local_1.predict(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to Improve?\n",
    "\n",
    "Look at **confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [7, 7]\n",
    "import matplotlib.pylab as plb\n",
    "plb.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "seq.plot_confusion_matrix(dev, local_1.predict(dev), normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Shows:\n",
    "\n",
    "* mostly strong diagonal (good predictions)\n",
    "* `NN` receives a lot of wrong counts, often confused with `NNP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "util.Carousel(local_1.errors(dev, \n",
    "                             filter_guess=lambda y: y=='NN',\n",
    "                             filter_gold=lambda y: y=='NNP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* \"Mayor\", \"Toussaint\" or \"H.I.V.\" are misclassified as common nouns \n",
    "* For $f_{\\text{word},w}$ feature template weights are $0$ \n",
    "\n",
    "Suggests that word has not appeared in the training set! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proper nouns tend to be capitalised!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def feat_2(x,i):\n",
    "    return {\n",
    "        'bias': 1.0,  \n",
    "        'word:' + x[i].lower(): 1.0,\n",
    "        'first_upper:' + str(x[i][0].isupper()): 1.0,\n",
    "    }\n",
    "local_2 = seq.LocalSequenceLabeler(feat_2, train)\n",
    "seq.accuracy(dev, local_2.predict(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Are these results actually caused by improved `NN`/`NNP` prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "seq.plot_confusion_matrix(dev, local_2.predict(dev), normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"../img/quiz_time.png\"></center>\n",
    "\n",
    "(Actually more \"brainstorm time\" ...)\n",
    "\n",
    "## https://tinyurl.com/nlp2019-pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named entity recognition (NER)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "| |\n",
    "|-|\n",
    "| \\[Barack Obama\\]<sub>PER</sub> was born in \\[Hawaii\\]<sub>LOC</sub> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NER as sequence labeling\n",
    "\n",
    "Label tokens as beginning (B), inside (I), or outside (O) a **named entity:**\n",
    "\n",
    "| | | | | | |\n",
    "|-|-|-|-|-|-|\n",
    "| Barack | Obama | was |  born | in | Hawaii |\n",
    "| B-PER | I-PER | O |  O | O | B-LOC |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Many tasks can be framed as sequence labeling using this idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GMB dataset also has named entity annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([tokens[12][:11], pos[12][:11], ents[12][:11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "examples = {}\n",
    "counts_ent = Counter(tag[2:] for sent in ents for tag in sent if tag.startswith(\"B-\"))\n",
    "in_entity = False\n",
    "for x_s, y_s in zip(tokens, ents):\n",
    "    for i, (x, y) in enumerate(zip(x_s, y_s)):\n",
    "        if y == \"O\":\n",
    "            in_entity = False\n",
    "            continue\n",
    "        y_ent = y[2:]\n",
    "        if y[0] == \"B\":\n",
    "            if y_ent not in examples or random() > 0.6:\n",
    "                examples[y_ent] = [x]\n",
    "                in_entity = True\n",
    "            else:\n",
    "                in_entity = False\n",
    "        if y[0] == \"I\" and in_entity:\n",
    "            examples[y_ent].append(x)\n",
    "\n",
    "sorted_ents = sorted(counts_ent.items(),key=lambda x:-x[1])\n",
    "sorted_ents_with_examples = [(t,c,\" \".join(examples[t])) for t,c in sorted_ents]\n",
    "\n",
    "sorted_ents_table = pd.DataFrame(sorted_ents_with_examples, columns=['Entity Type','Count','Example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sorted_ents_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can we run our simple **local model** on this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ner = list(zip(tokens[:-200], ents[:-200]))\n",
    "dev_ner = list(zip(tokens[-200:], ents[-200:]))\n",
    "\n",
    "def feat_2(x,i):\n",
    "    return {\n",
    "        'bias': 1.0,  \n",
    "        'word:' + x[i].lower(): 1.0,\n",
    "        'first_upper:' + str(x[i][0].isupper()): 1.0,\n",
    "    }\n",
    "local_2 = seq.LocalSequenceLabeler(feat_2, train_ner)\n",
    "seq.accuracy(dev_ner, local_2.predict(dev_ner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This seems great, but tag distribution is also **highly skewed**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_o = [tuple(['O'] * len(tags)) for _, tags in dev_ner]\n",
    "seq.accuracy(dev_ner, only_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_spans(labels):\n",
    "    spans = []\n",
    "    current = [None, None, None]\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.startswith(\"I-\") and label[2:] == current[0]:\n",
    "            # continued span\n",
    "            continue\n",
    "        # push span, if there is any\n",
    "        if current[0] is not None:\n",
    "            current[2] = i\n",
    "            spans.append(current)\n",
    "            current = [None, None, None]\n",
    "        if label.startswith(\"B-\"):\n",
    "            current[0] = label[2:]\n",
    "            current[1] = i\n",
    "    if current[0] is not None:\n",
    "        current[2] = len(labels)\n",
    "        spans.append(current)\n",
    "    return spans\n",
    "\n",
    "def _calculate_prf(preds, golds):\n",
    "    total_pred, total_gold, match = 0, 0, 0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        pred_s = get_spans(pred)\n",
    "        gold_s = get_spans(gold)\n",
    "        total_pred += len(pred_s)\n",
    "        total_gold += len(gold_s)\n",
    "        match += sum(s in pred_s for s in gold_s)\n",
    "    # precision: % of entities found by the system that are correct\n",
    "    p = match / total_pred\n",
    "    # recall: % of entities in dataset found by the system\n",
    "    r = match / total_gold\n",
    "    # f-score: harmonic mean of precision and recall\n",
    "    f = 2 * (p * r) / (p + r)\n",
    "\n",
    "    return p, r, f\n",
    "\n",
    "def calculate_prf(goldset, preds):\n",
    "    return _calculate_prf(preds, [s[1] for s in goldset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tasks like NER are more commonly evaluated with...\n",
    "\n",
    "### Precision, recall, and F-measure\n",
    "\n",
    "\\begin{align}\n",
    "\\text{precision} & = \\frac{|\\text{predicted}\\cap\\text{annotated}|}{|\\text{predicted}|} \\\\[.5em]\n",
    "\\text{recall} & = \\frac{|\\text{predicted}\\cap\\text{annotated}|}{|\\text{annotated}|} \\\\[.5em]\n",
    "F & = 2 \\cdot \\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calculate_prf(dev_ner, local_2.predict(dev_ner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence labeling with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically: Bi-directional RNNs, e.g. LSTMs\n",
    "\n",
    "\n",
    "<center>\n",
    "  <img src='../img/bplank_bilstm.png'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A recurrent neural network (plain RNN, LSTM, GRU, ...) computes its output based on a hidden, internal state:\n",
    "\n",
    "$$\n",
    " {\\mathbf{y}}_{t} = \\text{RNN}(\\x_t, {\\mathbf{h}}_{t})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "A **bi-directional** RNN is just two uni-directional RNNs combined:\n",
    "\n",
    "\\begin{align}\n",
    " \\overrightarrow{\\mathbf{y}_{t}} & = \\overrightarrow{\\text{RNN}}(\\x_t, \\overrightarrow{\\mathbf{h}_{t}})\\\\\n",
    " \\overleftarrow{\\mathbf{y}_{t}} & = \\overleftarrow{\\text{RNN}}(\\x_t, \\overleftarrow{\\mathbf{h}_{t}}) \n",
    " \\\\\n",
    " {\\mathbf{y}}_{t} & = \\overrightarrow{\\mathbf{y}_{t}} \\oplus \\overleftarrow{\\mathbf{y}_{t}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "+ $\\overrightarrow{\\text{RNN}}$ reads the input sequence $\\x$ from left to right\n",
    "+ $\\overleftarrow{\\text{RNN}}$ reads the input sequence $\\x$ from right to left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A different illustration of the same concept:\n",
    "\n",
    "<center>\n",
    "  <img style=\"width:28vw;\" src='../img/genthial_bilstm.png'/>\n",
    "</center>\n",
    "\n",
    "<span class=\"font-size:small;\">Source: https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To predict label probabilities, we use the **softmax function**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " {\\mathbf{y}}_{t} & = \\overrightarrow{\\mathbf{y}_{t}} \\oplus \\overleftarrow{\\mathbf{y}_{t}} \\\\\n",
    " \\hat{\\mathbf{y}}_{t} & = \\text{softmax}(\\mathbf{W}^o \\mathbf{y}_{t}) \\in \\mathbb{R}^{|V|} \\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An important technical detail\n",
    "\n",
    "The linear transformation $\\mathbf{W}^o \\mathbf{y}_{t}$ is usually not modelled as part of the RNN itself in most deep learning frameworks.\n",
    "\n",
    "Instead, look for one of\n",
    "\n",
    "+ **feed-forward layer**\n",
    "+ **dense layer** (*e.g. in Keras*)\n",
    "+ **linear layer** (*e.g. in PyTorch*)\n",
    "\n",
    "with a softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember the log-linear classifier:\n",
    "\n",
    "$$\n",
    "  p_\\params(y\\bar\\x,i) = \\frac{1}{Z_\\x} \\exp \\langle \\repr(\\x,i),\\params_y \\rangle\n",
    "$$\n",
    "\n",
    "A bi-LSTM model with a softmax layer on top is also modelling $p_\\params(y\\bar\\x,i)$, so if you take $\\params$ to be the set of parameters of the neural network, then:\n",
    "\n",
    "\\begin{align}\n",
    " \\hat{\\mathbf{y}}_{t} & = \\text{softmax}(\\hat{\\mathbf{h}}_{t}) \\\\\n",
    "  &= \\frac{1}{Z_\\x} \\exp \\langle \\hat{\\mathbf{h}}_{t},\\params_y \\rangle \\\\\n",
    "\\end{align}\n",
    "\n",
    "**What, then, are the most important differences to the simple log-linear approach?**\n",
    "\n",
    "## https://tinyurl.com/nlp2019-bilstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What haven't we modeled yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## There are *dependencies* between consecutive labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you think about fitting words for this POS tag sequence?\n",
    "\n",
    "| | | |\n",
    "|-|-|-|\n",
    "| DT | JJ | NN |\n",
    "| *determiner* | *adjective* | *noun (singular or mass)* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about this one?\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| DT | VB |\n",
    "| *determiner* | *verb (base form)* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ After determiners (`DT`), adjectives and nouns are much more likely than verbs\n",
    "+ *Local* models cannot *directly* capture this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "util.Carousel(local_2.errors(dev_ner, \n",
    "                             filter_guess=lambda y: y.startswith(\"I-\"),\n",
    "                             filter_gold=lambda y: y.startswith(\"B-\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the IOB tagging scheme:\n",
    "\n",
    "+ `I-[label]` can logically **only** appear after `B-[label]`!\n",
    "\n",
    "The following can **never** be valid tag sequences:\n",
    "\n",
    "* `O  I-per`\n",
    "\n",
    "* `B-per  I-geo`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if we went from this...\n",
    "\n",
    "$$\n",
    "p_\\params(\\y|\\x) = \\prod_{i=1}^n p_\\params(y_i|\\x,i)\n",
    "$$\n",
    "\n",
    "...to this?\n",
    "\n",
    "$$\n",
    "p_\\params(\\y|\\x) = \\prod_{i=1}^n p_\\params(y_i|\\x,y_{i-1},i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### First-order Markov assumption\n",
    "\n",
    "* Probability of a label depends only on the previous label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "$$\n",
    "\\prob_\\params(\\text{\"O I-per I-per\"} \\bar \\text{\"president Bill Clinton\"}) = \\\\\n",
    "\\prob_\\params(\\text{\"O\"}\\bar \\text{\"president Bill Clinton\"},\\text{\"<PAD>\"},1) ~ \\cdot \\\\\n",
    "\\prob_\\params(\\text{\"I-per\"} \\bar \\text{\"president Bill Clinton\"},\\text{\"O\"},2) ~ \\cdot \\\\\n",
    "\\prob_\\params(\\text{\"I-per\"} \\bar \\text{\"president Bill Clinton\"},\\text{\"I-per\"},3) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Entropy Markov Models (MEMM)\n",
    "\n",
    "Log-linear version with access to previous label: \n",
    "\n",
    "$$\n",
    "  p_\\params(y_i|\\x,y_{i-1},i) = \\frac{1}{Z_{\\x,y_{i-1},i}} \\exp \\langle \\repr(\\x,y_{i-1},i),\\params_{y_i} \\rangle\n",
    "$$\n",
    "\n",
    "where $Z_{\\x,y_{i-1},i}=\\sum_y \\exp \\langle \\repr(\\x,y_{i-1},i),\\params_{y_i} \\rangle $ is a *local* per-token normalisation factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training MEMMs\n",
    "Optimising the conditional likelihood \n",
    "\n",
    "$$\n",
    "\\sum_{(\\x,\\y) \\in \\train} \\log \\prob_\\params(\\y|\\x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Decomposes nicely: \n",
    "$$\n",
    "\\sum_{(\\x,\\y) \\in \\train} \\sum_{i=1}^{|\\x|} \\log \\prob_\\params(y_i|\\x,y_{i-1},i)   \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Easy to train\n",
    "* Equivalent to a **logistic regression objective** for a classifier that assigns labels based on previous gold labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However...\n",
    "\n",
    "### Local normalization introduces *label bias*\n",
    "\n",
    "+ Tag probabilities always sum to 1 at each position\n",
    "+ Can lead to MEMMs effectively \"ignoring\" the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Random Fields (CRF)\n",
    "\n",
    "Replace *local* with *global* normalization:\n",
    "\n",
    "$$\n",
    "  p_\\params(y_i|\\x,y_{i-1},i) = \\frac{1}{Z_{\\x}} \\exp \\langle \\repr(\\x,y_{i-1},i),\\params_{y_i} \\rangle\n",
    "$$\n",
    "\n",
    "where $Z_{\\x}=\\sum_\\y   \\prod_i^{|\\x|} \\exp \\langle \\repr(\\x,y_{i-1},i), \\params_{y_i} \\rangle$ is the *partition function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "+ More precisely, this is a **linear-chain CRF**.\n",
    "\n",
    "  (CRFs can be applied to any graph structure, but we are only considering sequences.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## &#128077;\n",
    "\n",
    "+ Finds globally optimal set of features\n",
    "+ Eliminates label bias\n",
    "\n",
    "## &#128078;\n",
    "\n",
    "+ More difficult to train (â€”cannot break down into local terms anymore!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The best of both worlds?\n",
    "\n",
    "## BiLSTM-CRF\n",
    "\n",
    "+ We can **combine** our bi-directional LSTM model with a CRF!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "  <img src=\"../img/ner_bilstm.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "  <img src=\"../img/ner_bilstm_crf.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction in MEMMs, CRFs, BiLSTM-CRFs, ...\n",
    "\n",
    "To predict the best label sequence, find a $\\y^*$ with maximal conditional probability\n",
    "\n",
    "$$\n",
    "\\y^* =\\argmax_\\y \\prob_\\params(\\y|\\x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Problem\n",
    "\n",
    "We cannot simply choose each label in isolation because **decisions depend on each other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Greedy prediction\n",
    "\n",
    "Simplest option:\n",
    "* Choose highest scoring label for token 1\n",
    "* Choose highest scoring label for token 2, conditioned on best label from 1\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But...\n",
    "\n",
    "+ May lead to **search errors** when returned $\\y^*$ is not highest scoring **global** solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Beam Search\n",
    "\n",
    "Keep a \"beam\" of the best $\\beta$ previous solutions\n",
    "\n",
    "1. Choose $\\beta$ highest scoring labels for token 1\n",
    "2. 1. For each of the previous $\\beta$ labels: Predict probabilities for next label, conditioned on the previous label(s)\n",
    "   2. **Sum** the probabilities for previous states and next label\n",
    "   3. **Prune** the beam by only keeping the top $\\beta$ paths\n",
    "3. Repeat until end of sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "* Many problems can be cast as sequence labeling\n",
    "* Solution 1: Sequence of linear regression classifiers\n",
    "    * Rely on good feature engineering\n",
    "* Solution 2: Recurrent neural networks (e.g., bidirectional LSTMs)\n",
    "    * Rely on substantial amounts of training data\n",
    "* Solution 3: CRFs to model label dependencies\n",
    "    * Can be stacked on top of neural networks\n",
    "    * Require non-trivial search algorithms\n",
    "    * ...but greedy and beam search often work well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background Material \n",
    "* Jurafsky & Martin, Speech and Language Processing, [Â§8.4 and Â§8.5](https://web.stanford.edu/~jurafsky/slp3/8.pdf) introduces Markov chains, HMMs, & MEMMs\n",
    "* Tutorial on CRFs: Sutton & McCallum, [An Introduction to Conditional Random Fields for Relational Learning](https://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf)\n",
    "* LSTM-CRF architecture: Huang et al., [Bidirectional LSTM-CRF for Sequence Tagging](https://arxiv.org/pdf/1508.01991v1.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 0.7\n",
    "#\n",
    "#   \\tikzset{state/.style={draw,rectangle,minimum height=1.5em,minimum width=2em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em,fill=gray!10},\n",
    "#            addstate/.style={draw,circle,inner sep=0.2em,fill=gray!10},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.2em},\n",
    "#            outer/.style={outer sep=0},\n",
    "#            label/.style={align=center,font=\\bfseries\\itshape\\small,text height=0.5em}}\n",
    "#\n",
    "#      \\node[emptystate]    (I1)    at  (0, 0.0)  {Barack};\n",
    "#      \\node[emptystate]    (I2)    at  (3, 0.0)  {Obama};\n",
    "#      \\node[emptystate]    (I3)    at  (6, 0.0)  {visits};\n",
    "#      \\node[emptystate]    (I4)    at  (9, 0.0)  {Denmark};\n",
    "#\n",
    "#      \\node[state]         (FRNN1)  at  (-0.5, 1.5)  {};\n",
    "#      \\node[state]         (FRNN2)  at  ( 2.5, 1.5)  {};\n",
    "#      \\node[state]         (FRNN3)  at  ( 5.5, 1.5)  {};\n",
    "#      \\node[state]         (FRNN4)  at  ( 8.5, 1.5)  {};\n",
    "#\n",
    "#      \\node[state]         (BRNN1)  at  (0.5, 3.0)  {};\n",
    "#      \\node[state]         (BRNN2)  at  (3.5, 3.0)  {};\n",
    "#      \\node[state]         (BRNN3)  at  (6.5, 3.0)  {};\n",
    "#      \\node[state]         (BRNN4)  at  (9.5, 3.0)  {};\n",
    "#\n",
    "#      \\node[addstate]      (A1)    at  (0, 4.5)  {$\\oplus$};\n",
    "#      \\node[addstate]      (A2)    at  (3, 4.5)  {$\\oplus$};\n",
    "#      \\node[addstate]      (A3)    at  (6, 4.5)  {$\\oplus$};\n",
    "#      \\node[addstate]      (A4)    at  (9, 4.5)  {$\\oplus$};\n",
    "#\n",
    "#      \\node[state]    (CRF1)    at  (0, 6.0)  {};\n",
    "#      \\node[state]    (CRF2)    at  (3, 6.0)  {};\n",
    "#      \\node[state]    (CRF3)    at  (6, 6.0)  {};\n",
    "#      \\node[state]    (CRF4)    at  (9, 6.0)  {};\n",
    "#\n",
    "#      \\node[emptystate]    (O1)    at  (0, 7.5)  {B-per};\n",
    "#      \\node[emptystate]    (O2)    at  (3, 7.5)  {I-per};\n",
    "#      \\node[emptystate]    (O3)    at  (6, 7.5)  {O};\n",
    "#      \\node[emptystate]    (O4)    at  (9, 7.5)  {B-geo};\n",
    "#\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\draw[->]          (I\\step) to (FRNN\\step);\n",
    "#        \\draw[->]          (I\\step) to (BRNN\\step);\n",
    "#        \\draw[->]          (FRNN\\step) to (A\\step);\n",
    "#        \\draw[->]          (BRNN\\step) to (A\\step);\n",
    "#        \\draw[->]          (A\\step) to (CRF\\step);\n",
    "#        \\draw[->]          (CRF\\step) to (O\\step);\n",
    "#      }\n",
    "#\n",
    "#      \\draw[->, densely dashed]  (FRNN1) to (FRNN2);\n",
    "#      \\draw[->, densely dashed]  (FRNN2) to (FRNN3);\n",
    "#      \\draw[->, densely dashed]  (FRNN3) to (FRNN4);\n",
    "#      \\draw[->, densely dashed]  (BRNN4) to (BRNN3);\n",
    "#      \\draw[->, densely dashed]  (BRNN3) to (BRNN2);\n",
    "#      \\draw[->, densely dashed]  (BRNN2) to (BRNN1);\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 0.7\n",
    "#\n",
    "#   \\tikzset{state/.style={draw,rectangle,minimum height=1.5em,minimum width=2em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em,fill=gray!10},\n",
    "#            crfstate/.style={draw,rectangle,minimum height=1.5em,minimum width=20em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em,fill=gray!10},\n",
    "#            addstate/.style={draw,circle,inner sep=0.2em,fill=gray!10},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.2em},\n",
    "#            outer/.style={outer sep=0},\n",
    "#            label/.style={align=center,font=\\bfseries\\itshape\\small,text height=0.5em}}\n",
    "#\n",
    "#      \\node[emptystate]    (I1)    at  (0, 0.0)  {Barack};\n",
    "#      \\node[emptystate]    (I2)    at  (3, 0.0)  {Obama};\n",
    "#      \\node[emptystate]    (I3)    at  (6, 0.0)  {visits};\n",
    "#      \\node[emptystate]    (I4)    at  (9, 0.0)  {Denmark};\n",
    "#\n",
    "#      \\node[state]         (FRNN1)  at  (-0.5, 1.5)  {};\n",
    "#      \\node[state]         (FRNN2)  at  ( 2.5, 1.5)  {};\n",
    "#      \\node[state]         (FRNN3)  at  ( 5.5, 1.5)  {};\n",
    "#      \\node[state]         (FRNN4)  at  ( 8.5, 1.5)  {};\n",
    "#\n",
    "#      \\node[state]         (BRNN1)  at  (0.5, 3.0)  {};\n",
    "#      \\node[state]         (BRNN2)  at  (3.5, 3.0)  {};\n",
    "#      \\node[state]         (BRNN3)  at  (6.5, 3.0)  {};\n",
    "#      \\node[state]         (BRNN4)  at  (9.5, 3.0)  {};\n",
    "#\n",
    "#      \\node[addstate]      (A1)    at  (0, 4.5)  {$\\oplus$};\n",
    "#      \\node[addstate]      (A2)    at  (3, 4.5)  {$\\oplus$};\n",
    "#      \\node[addstate]      (A3)    at  (6, 4.5)  {$\\oplus$};\n",
    "#      \\node[addstate]      (A4)    at  (9, 4.5)  {$\\oplus$};\n",
    "#\n",
    "#      \\node[crfstate]      (CRF)   at  (4.5, 6.0) {CRF};\n",
    "#      \\node[emptystate]    (CRF1)    at  (0, 6.0)  {};\n",
    "#      \\node[emptystate]    (CRF2)    at  (3, 6.0)  {};\n",
    "#      \\node[emptystate]    (CRF3)    at  (6, 6.0)  {};\n",
    "#      \\node[emptystate]    (CRF4)    at  (9, 6.0)  {};\n",
    "#\n",
    "#      \\node[emptystate]    (O1)    at  (0, 7.5)  {B-per};\n",
    "#      \\node[emptystate]    (O2)    at  (3, 7.5)  {I-per};\n",
    "#      \\node[emptystate]    (O3)    at  (6, 7.5)  {O};\n",
    "#      \\node[emptystate]    (O4)    at  (9, 7.5)  {B-geo};\n",
    "#\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\draw[->]          (I\\step) to (FRNN\\step);\n",
    "#        \\draw[->]          (I\\step) to (BRNN\\step);\n",
    "#        \\draw[->]          (FRNN\\step) to (A\\step);\n",
    "#        \\draw[->]          (BRNN\\step) to (A\\step);\n",
    "#        \\draw[->]          (A\\step) to (CRF\\step);\n",
    "#        \\draw[->]          (CRF\\step) to (O\\step);\n",
    "#      }\n",
    "#\n",
    "#      \\draw[->, densely dashed]  (FRNN1) to (FRNN2);\n",
    "#      \\draw[->, densely dashed]  (FRNN2) to (FRNN3);\n",
    "#      \\draw[->, densely dashed]  (FRNN3) to (FRNN4);\n",
    "#      \\draw[->, densely dashed]  (BRNN4) to (BRNN3);\n",
    "#      \\draw[->, densely dashed]  (BRNN3) to (BRNN2);\n",
    "#      \\draw[->, densely dashed]  (BRNN2) to (BRNN1);\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}